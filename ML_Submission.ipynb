{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "FJNUwmbgGyua",
        "-JiQyfWJYklI",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -**Glassdoor Tech Jobs Salary Prediction Using Machine Learning**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Supervised Machine Learning â€“ Regression Project\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary**\n",
        "\n",
        "Understanding salary trends in the technology industry has become increasingly important for job seekers, employers, recruiters, and policymakers in todayâ€™s competitive job market. With the rapid expansion of technology roles across various industries and geographies, compensation varies significantly based on job titles, company size, experience, and location. The objective of this project is to analyze a real-world dataset of job postings from Glassdoor (2017â€“2018) and develop a robust salary prediction system using machine learning. In addition to building a predictive model, the project focuses on uncovering patterns, trends, and insights that can help different stakeholders make informed decisions.\n",
        "\n",
        "The dataset includes a variety of features such as job title, company name, salary estimate, rating, company size, location, headquarters, founded year, industry, sector, revenue, and competitors. These attributes play an essential role in determining how compensation varies across organizations and regions. As a first step, the project performs detailed data cleaning, addressing missing values, removing outliers, and converting unstructured salary ranges into meaningful numerical values. Important feature engineering steps like extracting company age, location-based state labels, and job description length further enhance the dataset, enabling deeper analysis and better model performance.\n",
        "\n",
        "The exploratory data analysis (EDA) phase focuses on identifying trends in salary variations across job titles, cities, company attributes, and industries. With the help of visualizations such as histograms, bar charts, boxplots, scatter plots, and correlation heatmaps, we analyze how salaries differ among roles like Data Scientist, Software Engineer, Machine Learning Engineer, and DevOps Engineer. The project also evaluates whether larger companies tend to offer higher compensation and how location impacts salary disparities, especially in tech hubs such as California, New York, and Washington. This phase helps validate key hypotheses, such as â€œData Science roles earn more than traditional software rolesâ€ and â€œCompany size and revenue contribute positively to salary levels.â€\n",
        "\n",
        "After completing the EDA, the project transitions into predictive modeling. A set of supervised regression algorithmsâ€”including Linear Regression as a baseline and Random Forest Regressor as an advanced modelâ€”are trained to predict the average salary for a given job listing. Categorical features are encoded using OneHotEncoding, and numerical variables are processed appropriately to ensure smooth model training. Hyperparameter tuning using GridSearchCV is performed to optimize the Random Forest model for better accuracy and generalization. Model performance is evaluated using metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and RÂ² score.\n",
        "\n",
        "The model comparison reveals that Random Forest with hyperparameter tuning significantly outperforms Linear Regression, achieving higher prediction accuracy and capturing complex feature interactions. Additionally, feature importance analysis highlights the most influential factors affecting salary, such as job title, location, company size, rating, and industry type. These insights provide valuable information to stakeholders. Job seekers can better understand expected compensation ranges before applying for roles, employers can benchmark their salary offerings to stay competitive, and recruiters can attract top talent by setting appropriate salary bands. Policy analysts can also use the insights to study wage inequalities across industries and regions.\n",
        "\n",
        "Overall, this project highlights the power of machine learning in extracting meaningful salary trends from real-world job data. By combining effective data preprocessing, exploratory analysis, feature engineering, predictive modeling, and insight generation, the project delivers a comprehensive solution for salary prediction and market trend analysis. This analysis not only supports accurate compensation forecasting but also helps different stakeholders navigate the dynamic and fast-evolving tech job landscape with clarity and confidence."
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The technology job market is highly dynamic, with salaries varying significantly across job roles, locations, company sizes, and industry sectors. Job seekers often struggle to understand what salary range to expect for a specific role, while employers and recruiters must continually benchmark compensation to remain competitive. Despite the availability of job postings on platforms like Glassdoor, the information is often unstructured, inconsistent, and difficult to interpret without analytical tools.\n",
        "\n",
        "The goal of this project is to analyze historical Glassdoor job posting data (2017â€“2018) and build a predictive machine learning model capable of estimating salaries for various tech job roles. This involves cleaning and transforming raw job posting attributes, extracting meaningful features, performing exploratory data analysis to uncover salary patterns, and applying regression algorithms to predict salary values accurately.\n",
        "\n",
        "The project seeks to answer the following key questions:\n",
        "\n",
        "How do salaries differ across job positions such as Data Scientist, Software Engineer, or DevOps Engineer?\n",
        "\n",
        "What impact do company characteristicsâ€”such as size, rating, and revenueâ€”have on salary levels?\n",
        "\n",
        "How do geographic factors influence compensation, particularly across major tech hubs?\n",
        "\n",
        "Which job attributes play the most significant role in determining salary?\n",
        "\n",
        "Can machine learning models generalize well enough to predict salary ranges for unseen job listings?\n",
        "\n",
        "Ultimately, the problem is to transform unstructured compensation data into actionable insights and accurate predictions. The solution should help stakeholdersâ€”including job seekers, employers, recruiters, and analystsâ€”better understand salary expectations, optimize hiring strategies, and make data-driven decisions in the competitive tech industry."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "# Core numerical & data manipulation libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure basic visualization settings\n",
        "plt.style.use(\"default\")     # use default style for consistency\n",
        "sns.set(font_scale=1.0)      # make plots readable in the notebook\n",
        "\n",
        "# Ignore harmless warnings to keep output clean (optional)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"Libraries imported successfully.\")"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "\n",
        "# Using a small helper + try/except to make it production friendly\n",
        "def load_glassdoor_data(filepath: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Loads Glassdoor jobs dataset from the given CSV filepath.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filepath : str\n",
        "        Relative or absolute path to the CSV file.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    df : pd.DataFrame\n",
        "        Loaded dataset as a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "        print(f\"Dataset loaded successfully from: {filepath}\")\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at path -> {filepath}\")\n",
        "        raise\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"Error: The file is empty.\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error while loading dataset: {e}\")\n",
        "        raise\n",
        "\n",
        "# NOTE: make sure the CSV is in the same folder OR give full path\n",
        "data_path = \"glassdoor_jobs.csv\"\n",
        "df = load_glassdoor_data(data_path)\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "\n",
        "n_rows, n_cols = df.shape\n",
        "print(f\"Number of rows   : {n_rows}\")\n",
        "print(f\"Number of columns: {n_cols}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "\n",
        "print(\"Basic information about the dataset:\")\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "# Count total duplicate rows\n",
        "dup_count = df.duplicated().sum()\n",
        "print(f\"Total duplicate rows in the dataset: {dup_count}\")\n",
        "\n",
        "# If duplicates exist, we can drop them (keeping first occurrence)\n",
        "if dup_count > 0:\n",
        "    df = df.drop_duplicates().reset_index(drop=True)\n",
        "    print(f\"Duplicates dropped. New shape: {df.shape}\")\n",
        "else:\n",
        "    print(\"No duplicate rows found.\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values / Null Values\n",
        "\n",
        "# Calculate count of missing values per column\n",
        "missing_count = df.isna().sum()\n",
        "\n",
        "# Calculate percentage of missing values per column\n",
        "missing_percent = (missing_count / len(df)) * 100\n",
        "\n",
        "# Combine both into a single DataFrame for better readability\n",
        "missing_df = pd.DataFrame({\n",
        "    \"missing_count\": missing_count,\n",
        "    \"missing_percent\": missing_percent.round(2)\n",
        "}).sort_values(by=\"missing_percent\", ascending=False)\n",
        "\n",
        "print(\"Missing values (count & % by column):\")\n",
        "missing_df"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing missing values pattern (helpful for EDA decisions)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.isna(), cbar=False)\n",
        "plt.title(\"Missing Values Heatmap\")\n",
        "plt.xlabel(\"Columns\")\n",
        "plt.ylabel(\"Rows\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Glassdoor Jobs dataset contains real-world job posting information collected from Glassdoor.com during the years 2017â€“2018. The dataset primarily focuses on various tech-related job roles, company attributes, and salary estimates. Its primary objective is to help analyze how different job attributes influence salary variations across the technology sector.\n",
        "\n",
        "**Key Things Understood About the Dataset**\n",
        "1. Dataset Structure\n",
        "\n",
        "The dataset is in CSV format and contains multiple columns describing job postings.\n",
        "\n",
        "It has rows representing individual job listings.\n",
        "\n",
        "It includes a mix of categorical, numerical, and text fields.\n",
        "\n",
        "The dataset contains columns such as:\n",
        "\n",
        "Job Title\n",
        "\n",
        "Salary Estimate / Min Salary / Max Salary / Avg Salary\n",
        "\n",
        "Company Name\n",
        "\n",
        "Company Size\n",
        "\n",
        "Company Rating\n",
        "\n",
        "Location\n",
        "\n",
        "Founded Year\n",
        "\n",
        "Industry & Sector\n",
        "\n",
        "Revenue\n",
        "\n",
        "Competitors\n",
        "\n",
        "Job Description length (sdesc_len)\n",
        "\n",
        "Other engineered features like num_comp\n",
        "\n",
        "These attributes can significantly influence salary trends.\n",
        "\n",
        "2. Insights About Data Types\n",
        "\n",
        "Many columns are categorical (job_title, location, size, industry, sector, revenue).\n",
        "\n",
        "Some are numerical (rating, founded, avg_salary, hourly_min, hourly_max, num_comp).\n",
        "\n",
        "Some are text-based (job_description, competitors).\n",
        "\n",
        "Some contain mixed formats such as salary ranges (\"$85K-$120K\") that must be cleaned.\n",
        "\n",
        "3. Presence of Missing Values\n",
        "\n",
        "Several columns contain missing or null values, especially:\n",
        "\n",
        "competitors\n",
        "\n",
        "revenue\n",
        "\n",
        "industry\n",
        "\n",
        "sector\n",
        "\n",
        "salary_estimate\n",
        "\n",
        "These must be cleaned, imputed, or dropped depending on their importance and missing percentage.\n",
        "\n",
        "4. Presence of Duplicate or Redundant Entries\n",
        "\n",
        "Initial exploration shows some duplicates, which must be removed to avoid bias in ML modeling.\n",
        "\n",
        "5. Salary Data is Unclean\n",
        "\n",
        "Salary values are often stored as text ranges (e.g., \"$50Kâ€“$90K\").\n",
        "\n",
        "These values must be:\n",
        "\n",
        "Cleaned\n",
        "\n",
        "Converted to numeric\n",
        "\n",
        "Split into min, max, and average salary\n",
        "\n",
        "Some rows contain hourly salary formats (e.g., \"$40 / hr\") which also need standardization.\n",
        "\n",
        "6. Feature Engineering Possibilities\n",
        "\n",
        "The dataset gives multiple opportunities for creating new features:\n",
        "\n",
        "Company Age = Current Year â€“ Founded Year\n",
        "\n",
        "State Extraction from location (City, State)\n",
        "\n",
        "Job Description Length helps determine job complexity\n",
        "\n",
        "Revenue Bins (Small, Medium, Large companies)\n",
        "These features will help improve model performance.\n",
        "\n",
        "7. Dataset Contains Both Business & Technical Information\n",
        "\n",
        "Business attributes: revenue, company size, company age\n",
        "\n",
        "Technical attributes: job description length, job title\n",
        "\n",
        "Compensation attributes: salary estimate, avg_salary\n",
        "\n",
        "This mix allows both salary trend analysis and predictive modeling.\n",
        "\n",
        "8. Suitable for Regression Machine Learning Models\n",
        "\n",
        "Since salary is a continuous numeric target, the dataset is perfect for:\n",
        "\n",
        "Linear Regression\n",
        "\n",
        "Random Forest Regressor\n",
        "\n",
        "XGBoost\n",
        "\n",
        "Lasso / Ridge Regression\n",
        "\n",
        "9. Dataset is Larger and Diverse\n",
        "\n",
        "Contains multiple job categories such as:\n",
        "\n",
        "Data Scientist\n",
        "\n",
        "Software Engineer\n",
        "\n",
        "DevOps Engineer\n",
        "\n",
        "Business Analyst\n",
        "\n",
        "ML Engineer\n",
        "\n",
        "Covers multiple US states, giving strong geographical variation.\n",
        "\n",
        "10. Real-World Practical Use\n",
        "\n",
        "The dataset is close to what companies actually publish in job advertisements.\n",
        "This makes it ideal for:\n",
        "\n",
        "- Salary benchmarking\n",
        "- Market trend prediction\n",
        "- Career guidance\n",
        "- HR analytics\n",
        "- Building data-driven salary recommendation systems"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "\n",
        "print(\"List of all columns in the dataset:\\n\")\n",
        "for col in df.columns:\n",
        "    print(col)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "\n",
        "print(\"Statistical summary of numerical columns:\\n\")\n",
        "df.describe().T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Variable Name             | Description                                                                         |\n",
        "| ------------------------- | ----------------------------------------------------------------------------------- |\n",
        "| **job_title**             | The role or position advertised (e.g., Data Scientist, Software Engineer).          |\n",
        "| **salary_estimate**       | The salary range listed in the job posting (text format, needs cleaning).           |\n",
        "| **avg_salary**            | Engineered variable representing the average salary extracted from salary_estimate. |\n",
        "| **company_name**          | Name of the company posting the job.                                                |\n",
        "| **rating**                | Glassdoor rating of the company (0â€“5 scale).                                        |\n",
        "| **location**              | Location of the job (City, State format).                                           |\n",
        "| **headquarters**          | Companyâ€™s head office location.                                                     |\n",
        "| **size**                  | The employee count of the company (e.g., â€œ1001â€“5000 employeesâ€).                    |\n",
        "| **founded**               | Year the company was founded.                                                       |\n",
        "| **company_age**           | Engineered feature: (Current Year â€“ Founded Year).                                  |\n",
        "| **type_of_ownership**     | Ownership category (e.g., Private, Public, Government).                             |\n",
        "| **industry**              | The industry the company belongs to (e.g., IT Services, Finance).                   |\n",
        "| **sector**                | Business sector (broader category compared to industry).                            |\n",
        "| **revenue**               | Annual company revenue (e.g., â€œ$1Bâ€“$10Bâ€).                                          |\n",
        "| **competitors**           | List of competitor companies (text).                                                |\n",
        "| **num_comp**              | Number of competitors (engineered feature).                                         |\n",
        "| **sdesc_len**             | Length of the job description (engineered feature).                                 |\n",
        "| **hourly_min/hourly_max** | If the job is hourly-based, these represent salary range per hour.                  |\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique Values for Each Variable\n",
        "\n",
        "print(\"Unique value counts for each column:\\n\")\n",
        "\n",
        "unique_values = {}\n",
        "\n",
        "for col in df.columns:\n",
        "    unique_count = df[col].nunique()\n",
        "    unique_values[col] = unique_count\n",
        "    print(f\"{col:25} --> {unique_count}\")\n",
        "\n",
        "# Display as DataFrame for clarity\n",
        "pd.DataFrame(unique_values.items(), columns=[\"Column Name\", \"Unique Values\"])"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------\n",
        "# ðŸ§¹ DATA CLEANING & PREPARATION â€“ MAKE DATA ANALYSIS READY\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "# 1. Remove duplicates\n",
        "print(\"Duplicate rows before cleaning:\", df.duplicated().sum())\n",
        "df = df.drop_duplicates().reset_index(drop=True)\n",
        "print(\"Duplicate rows after cleaning :\", df.duplicated().sum())\n",
        "\n",
        "\n",
        "# 2. Standardize column names (optional but better for modeling)\n",
        "df.columns = df.columns.str.lower().str.replace(\" \", \"_\")\n",
        "\n",
        "\n",
        "# 3. Handle Missing Values\n",
        "# -----------------------------------------------------------\n",
        "# Strategy:\n",
        "# - Categorical variables â†’ fill with mode\n",
        "# - Numerical variables   â†’ fill with median\n",
        "# - Columns with >40% missing values â†’ dropped (low utility)\n",
        "\n",
        "missing_percent = df.isna().mean() * 100\n",
        "cols_to_drop = missing_percent[missing_percent > 40].index.tolist()\n",
        "\n",
        "print(\"\\nColumns dropped due to high missing %:\", cols_to_drop)\n",
        "df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "# Fill numerical missing values\n",
        "num_cols = df.select_dtypes(include=['int64','float64']).columns\n",
        "for col in num_cols:\n",
        "    df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "# Fill categorical missing values\n",
        "cat_cols = df.select_dtypes(include=['object']).columns\n",
        "for col in cat_cols:\n",
        "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "print(\"\\nMissing values after cleaning:\")\n",
        "print(df.isna().sum().sum(), \"missing values remaining\")\n",
        "\n",
        "\n",
        "# 4. Clean Salary Column â†’ Extract avg salary\n",
        "# -----------------------------------------------------------\n",
        "# Some rows may contain \"$XXK-$YYK\" â†’ we convert to numeric\n",
        "\n",
        "def clean_salary(val):\n",
        "    \"\"\"\n",
        "    Cleans and converts salary text into a numeric average salary.\n",
        "    Handles various formats including ranges, single values, hourly rates,\n",
        "    and removes extraneous text like 'Glassdoor est.' and 'Employer Provided Salary:'.\n",
        "    \"\"\"\n",
        "    if not isinstance(val, str):\n",
        "        return np.nan\n",
        "\n",
        "    # Clean common extraneous text\n",
        "    val = val.replace(\"(Glassdoor est.)\", \"\").replace(\"Employer Provided Salary:\", \"\").strip()\n",
        "    # Handle hourly salaries before removing 'K' as 'K' might be part of an hourly rate description\n",
        "    if \"per hour\" in val.lower():\n",
        "        # Assuming typical full-time work (40 hours/week, 52 weeks/year)\n",
        "        try:\n",
        "            hourly_rate = float(val.lower().replace(\"per hour\", \"\").replace(\"$\", \"\").strip())\n",
        "            return hourly_rate * 40 * 52 / 1000 # Convert to K (thousand)\n",
        "        except ValueError:\n",
        "            return np.nan\n",
        "\n",
        "    # Now remove '$' and 'K' for standard salary ranges\n",
        "    val = val.replace(\"$\", \"\").replace(\"K\", \"\").strip()\n",
        "\n",
        "    # Handle salary ranges (e.g., \"53-91\")\n",
        "    if \"-\" in val:\n",
        "        try:\n",
        "            low, high = val.split(\"-\")\n",
        "            low = float(low.strip())\n",
        "            high = float(high.strip())\n",
        "            return (low + high) / 2\n",
        "        except ValueError:\n",
        "            return np.nan\n",
        "    # Handle single values (e.g., \"91\")\n",
        "    else:\n",
        "        try:\n",
        "            return float(val.strip())\n",
        "        except ValueError:\n",
        "            return np.nan\n",
        "\n",
        "\n",
        "# If avg_salary already exists, skip cleaning\n",
        "if 'avg_salary' not in df.columns:\n",
        "    df['avg_salary'] = df['salary_estimate'].apply(clean_salary)\n",
        "\n",
        "# Drop rows where salary could not be extracted\n",
        "df = df[~df['avg_salary'].isna()].reset_index(drop=True)\n",
        "\n",
        "\n",
        "# 5. Create Company Age Feature\n",
        "# -----------------------------------------------------------\n",
        "if 'founded' in df.columns:\n",
        "    df['company_age'] = df['founded'].apply(lambda x: 2025 - x if x > 0 else np.nan)\n",
        "    df['company_age'].fillna(df['company_age'].median(), inplace=True)\n",
        "\n",
        "\n",
        "# 6. Extract State from Location (City, State)\n",
        "# -----------------------------------------------------------\n",
        "if 'location' in df.columns:\n",
        "    df['state'] = df['location'].apply(\n",
        "        lambda x: x.split(\",\")[-1].strip() if isinstance(x, str) and \",\" in x else x\n",
        "    )\n",
        "\n",
        "\n",
        "# 7. Create Job Description Length Feature\n",
        "# -----------------------------------------------------------\n",
        "if 'job_description' in df.columns:\n",
        "    df['sdesc_len'] = df['job_description'].apply(lambda x: len(str(x)))\n",
        "\n",
        "\n",
        "# 8. Remove Outliers from Salary using IQR\n",
        "# -----------------------------------------------------------\n",
        "Q1 = df['avg_salary'].quantile(0.25)\n",
        "Q3 = df['avg_salary'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower = Q1 - 1.5 * IQR\n",
        "upper = Q3 + 1.5 * IQR\n",
        "\n",
        "before = df.shape[0]\n",
        "df = df[(df['avg_salary'] >= lower) & (df['avg_salary'] <= upper)]\n",
        "after = df.shape[0]\n",
        "\n",
        "print(f\"\\nOutliers removed: {before - after}\")\n",
        "print(f\"Final dataset shape: {df.shape}\")"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Manipulations Performed**\n",
        "1. Removed Duplicate Rows\n",
        "\n",
        "Duplicate job entries were removed to avoid model bias and ensure each job listing is unique.\n",
        "\n",
        "2. Standardized Column Names\n",
        "\n",
        "Converted all column names to lowercase and replaced spaces with underscores for consistent processing.\n",
        "\n",
        "3. Cleaned Missing Values\n",
        "\n",
        "Dropped columns with >40% missing values (low information value).\n",
        "\n",
        "Filled:\n",
        "\n",
        "numerical columns with median\n",
        "\n",
        "categorical columns with mode\n",
        "\n",
        "This prevented model errors and ensured complete data.\n",
        "\n",
        "4. Cleaned and Converted Salary Values\n",
        "\n",
        "Salary estimates in formats like $60K-$120K were cleaned and converted into a numeric avg_salary variable.\n",
        "This allowed regression models to use salary as a continuous target.\n",
        "\n",
        "5. Created Important New Features\n",
        "\n",
        "company_age = 2025 - founded\n",
        "Older companies tend to offer more stable salaries.\n",
        "\n",
        "state extracted from location\n",
        "Helps in location-based analysis.\n",
        "\n",
        "sdesc_len (job description length)\n",
        "Longer descriptions often indicate more demanding roles.\n",
        "\n",
        "6. Handled Outliers Using IQR\n",
        "\n",
        "Extreme salary values were removed to prevent model distortion.\n",
        "This increased model performance and accuracy."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart â€“ 1: Distribution of Average Salary"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 : Distribution of Average Salary (Univariate)\n",
        "\n",
        "plt.figure(figsize=(9,5))\n",
        "sns.histplot(df['avg_salary'], kde=True, bins=30)\n",
        "plt.title(\"Distribution of Average Salary\", fontsize=14)\n",
        "plt.xlabel(\"Average Salary (in K)\", fontsize=12)\n",
        "plt.ylabel(\"Frequency\", fontsize=12)\n",
        "plt.grid(axis='y', linestyle=\"--\", alpha=0.4)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected the Histogram with KDE (Kernel Density Estimate) because it is one of the best visualizations for understanding the distribution of a continuous numerical variable.\n",
        "\n",
        "In this project, avg_salary is the target variable for modeling.\n",
        "Understanding its distribution is essential because:\n",
        "\n",
        "It helps identify skewness (left/right skewed)\n",
        "\n",
        "Shows salary concentration, such as the most common salary range\n",
        "\n",
        "Helps detect outliers graphically\n",
        "\n",
        "Guides the choice of ML algorithms (linear models assume normality; tree models donâ€™t)\n",
        "\n",
        "Helps to select appropriate data transformations\n",
        "\n",
        "So this chart is the most meaningful first step in any salary prediction project."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the distribution chart:\n",
        "\n",
        "The salary distribution is right-skewed (positively skewed).\n",
        "This means most job listings fall in the lower to mid salary range, and only a few roles have very high salaries.\n",
        "\n",
        "The most common salary range appears around 70K dollorsâ€“ 120K.dollors\n",
        "\n",
        "There are fewer extremely high salary values, indicating specialized or senior positions.\n",
        "\n",
        "The distribution is not perfectly normal; it has a long tail, which suggests the presence of outliers.\n",
        "\n",
        "The majority of tech job salaries cluster around a predictable range, making regression models suitable."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact**\n",
        "\n",
        "Yes, this insight has multiple positive business impacts:\n",
        "\n",
        "Job Seekers:\n",
        "Helps them understand the typical salary range, set realistic expectations, and negotiate confidently.\n",
        "\n",
        "Recruiters / HR Teams:\n",
        "They can benchmark salaries against industry norms.\n",
        "If their salary offering is below the common range, they risk losing talent â€” this insight helps them adjust.\n",
        "\n",
        "Companies / Employers:\n",
        "Understanding where salaries cluster helps them stay competitive and retain skilled tech talent.\n",
        "\n",
        "Data Scientists:\n",
        "Knowing skewness guides decisions on model selection, transformations, and handling outliers, which improves prediction accuracy.\n"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart â€“ 2: Boxplot of Average Salary"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 : Boxplot of Average Salary to Detect Outliers\n",
        "\n",
        "\n",
        "plt.figure(figsize=(9,4))\n",
        "sns.boxplot(x=df['avg_salary'])\n",
        "plt.title(\"Boxplot of Average Salary\", fontsize=14)\n",
        "plt.xlabel(\"Average Salary (in K)\", fontsize=12)\n",
        "plt.grid(axis='x', linestyle=\"--\", alpha=0.4)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected a boxplot because it is the most effective visualization for:\n",
        "\n",
        "Identifying outliers\n",
        "\n",
        "Understanding salary spread (min, Q1, median, Q3, max)\n",
        "\n",
        "Comparing central tendency and variability\n",
        "\n",
        "Detecting extreme values that may negatively impact model performance\n",
        "\n",
        "Since avg_salary is the target variable for machine learning regression, knowing whether the data contains extreme outliers is essential.\n",
        "\n",
        "Outliers can:\n",
        "\n",
        "Distort regression coefficients\n",
        "\n",
        "Reduce model accuracy\n",
        "\n",
        "Mislead insights\n",
        "\n",
        "Lead to incorrect salary predictions\n",
        "\n",
        "Therefore, a boxplot is the ideal tool to detect these anomalies."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key insights from the boxplot:\n",
        "\n",
        "The dataset contains multiple high-end salary outliers (right-side points beyond the whiskers).\n",
        "These likely represent senior roles, specialized tech roles, or high-paying locations (e.g., California).\n",
        "\n",
        "The median salary lies between $80Kâ€“$110K, confirming the salary concentration we observed in Chartâ€“1.\n",
        "\n",
        "The salary distribution is positively skewed, showing that most roles fall in mid salary ranges, with fewer high-paying roles stretching the tail.\n",
        "\n",
        "The presence of outliers suggests salary variations across roles and locations.\n",
        "\n",
        "These insights reinforce the need for outlier treatment before building models."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact**\n",
        "\n",
        "Yes, these insights contribute positively:\n",
        "\n",
        "For ML Model Development:\n",
        "Knowing outliers exist allows us to apply IQR-based filtering or log transformation.\n",
        "This results in:\n",
        "\n",
        "Better model stability\n",
        "\n",
        "Lower error\n",
        "\n",
        "More accurate salary predictions\n",
        "\n",
        "For Companies:\n",
        "Identifying extremely high salaries helps companies understand where the top-paying roles lie and optimize budget allocation.\n",
        "\n",
        "For Job Seekers:\n",
        "Understanding outliers helps candidates differentiate between typical salaries and exceptional ones.\n",
        "\n",
        "**Negative Growth Impact (If ignored)**\n",
        "\n",
        "If outliers are not treated:\n",
        "\n",
        "Models may overfit to extreme salary ranges.\n",
        "\n",
        "Prediction errors will increase dramatically.\n",
        "\n",
        "Businesses relying on salary predictions may make poor decisions (e.g., salary budgeting, hiring strategy).\n",
        "\n",
        "Job seekers may get unrealistic salary expectations.\n",
        "\n",
        "Outliers can distort insights, so detecting them early is crucial."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart â€“ 3: Average Salary by Job Title (Top 10 Roles)"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 : Average Salary by Job Title (Top 10)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "# Compute average salary for each job title\n",
        "top_titles = (df.groupby('job_title')['avg_salary']\n",
        "                .mean()\n",
        "                .sort_values(ascending=False)\n",
        "                .head(10))\n",
        "\n",
        "# Bar plot\n",
        "sns.barplot(x=top_titles.values, y=top_titles.index, palette=\"viridis\")\n",
        "\n",
        "plt.title(\"Top 10 Job Titles by Average Salary\", fontsize=14)\n",
        "plt.xlabel(\"Average Salary (in K)\", fontsize=12)\n",
        "plt.ylabel(\"Job Title\", fontsize=12)\n",
        "plt.grid(axis='x', linestyle=\"--\", alpha=0.3)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected a bar chart because it is the most effective visualization for comparing:\n",
        "\n",
        "A numerical variable (avg_salary)\n",
        "\n",
        "Against a categorical variable (job_title)\n",
        "\n",
        "Since job title is one of the primary drivers of salary, understanding how different roles vary in compensation is extremely important.\n",
        "A bar chart clearly shows:\n",
        "\n",
        "Salary differences between roles\n",
        "\n",
        "Top-paying job titles\n",
        "\n",
        "Overall market trends in the tech industry\n",
        "\n",
        "It is visually intuitive and perfect for storytelling, especially when focusing on the Top 10 job titles."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key insights from this chart:\n",
        "\n",
        "Data Scientist, Machine Learning Engineer, and Senior Software Engineer are among the highest-paying roles.\n",
        "\n",
        "Entry or mid-level engineering roles have comparatively lower average salaries.\n",
        "\n",
        "Specialized technical roles (ML, AI, Data Science, Cloud Engineering) clearly command higher pay.\n",
        "\n",
        "The salary difference between top and bottom job roles is significant, indicating a strong variance based on skill specialization.\n",
        "\n",
        "This confirms that job title is a major factor influencing salary."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact**\n",
        "\n",
        "Yes, the insights can significantly help:\n",
        "\n",
        "Job Seekers:\n",
        "Understand which roles offer higher salaries and choose career paths accordingly.\n",
        "\n",
        "Recruiters / HR Managers:\n",
        "Benchmark their salary offerings to stay competitive and attract top talent.\n",
        "\n",
        "Companies:\n",
        "Know which high-demand roles require higher budgets.\n",
        "\n",
        "Education & Training Institutes:\n",
        "Identify trending roles and customize skill-based programs.\n",
        "\n",
        "**Negative Growth Impact (If insights are ignored)**\n",
        "\n",
        "If businesses ignore these insights:\n",
        "\n",
        "They may underpay high-demand roles (like ML or Data Science), leading to:\n",
        "\n",
        "High attrition\n",
        "\n",
        "Failed hiring cycles\n",
        "\n",
        "Loss of skilled professionals\n",
        "\n",
        "Job seekers may choose career paths with lower pay due to lack of awareness.\n",
        "\n",
        "Companies may misallocate salary budgets into low-impact areas.\n",
        "\n",
        "So the chart provides valuable direction for strategic decision-making."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart â€“ 4: Average Salary by Company Size"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 : Average Salary by Company Size\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "# Calculate mean salary by company size\n",
        "size_salary = (df.groupby('size')['avg_salary']\n",
        "                 .mean()\n",
        "                 .sort_values(ascending=False))\n",
        "\n",
        "sns.barplot(x=size_salary.values, y=size_salary.index, palette=\"magma\")\n",
        "\n",
        "plt.title(\"Average Salary by Company Size\", fontsize=14)\n",
        "plt.xlabel(\"Average Salary (in K)\", fontsize=12)\n",
        "plt.ylabel(\"Company Size Category\", fontsize=12)\n",
        "plt.grid(axis='x', linestyle=\"--\", alpha=0.3)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a bar chart because it is the most effective way to compare numerical values across different company size categories, which are categorical in nature.\n",
        "This chart helps clearly visualize how salary levels vary across companies of different workforce sizes.\n",
        "\n",
        "The goal is to determine whether larger companies pay more, or if smaller companies have unique advantages.\n",
        "Bar charts make such comparisons easy, intuitive, and visually clear."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key insights from this chart:\n",
        "\n",
        "Large companies such as those with 10,000+ employees and 5001â€“10,000 employees offer significantly higher average salaries.\n",
        "\n",
        "Mid-sized companies also pay competitive salaries, but not as high as very large organizations.\n",
        "\n",
        "Smaller companies (e.g., 1â€“50 employees, 51â€“200 employees) tend to offer lower average salaries.\n",
        "\n",
        "This clearly indicates a trend:\n",
        "Company size is positively correlated with salary.\n",
        "\n",
        "Possible reasons include:\n",
        "\n",
        "Large companies have bigger budgets\n",
        "\n",
        "Stability and revenue strength\n",
        "\n",
        "More structured compensation frameworks\n",
        "\n",
        "Higher demand for specialized tech talent"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ” Positive Business Impact\n",
        "\n",
        "Yes, this insight contributes significantly:\n",
        "\n",
        "Job Seekers:\n",
        "Can understand that larger companies usually offer higher pay.\n",
        "Helps them target organizations based on salary expectations.\n",
        "\n",
        "Employers / HR Teams:\n",
        "Smaller companies can benchmark themselves and adjust salaries to stay competitive.\n",
        "\n",
        "Recruitment Strategy:\n",
        "Companies can optimize job postings and budgets based on size-wise salary trends.\n",
        "\n",
        "Business Analysts:\n",
        "Helps forecast compensation trends based on organizational growth.\n",
        "\n",
        "â— Negative Growth Impact (If insights are ignored)\n",
        "\n",
        "Ignoring these insights may lead to:\n",
        "\n",
        "Small companies consistently losing talent to large companies.\n",
        "\n",
        "Incorrect salary budgeting, leading to hiring delays and skill gaps.\n",
        "\n",
        "Startups offering very low salaries may struggle to attract experienced engineers.\n",
        "\n",
        "ML model may mispredict salaries if company size is not included, reducing accuracy and harming decision-making.\n",
        "\n",
        "Thus, company size is a crucial factor for both business strategy and prediction accuracy."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart â€“ 5: Average Salary by Location (Top 10 States)"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 : Average Salary by Location (Top 10 States)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "# Ensure 'state' column exists (created earlier from 'location')\n",
        "if 'state' in df.columns:\n",
        "    location_col = 'state'\n",
        "else:\n",
        "    location_col = 'location'\n",
        "\n",
        "# Compute average salary for each state\n",
        "top_locations = (df.groupby(location_col)['avg_salary']\n",
        "                   .mean()\n",
        "                   .sort_values(ascending=False)\n",
        "                   .head(10))\n",
        "\n",
        "# Bar chart\n",
        "sns.barplot(x=top_locations.values, y=top_locations.index, palette=\"coolwarm\")\n",
        "\n",
        "plt.title(\"Top 10 Highest Paying States (Avg Salary)\", fontsize=14)\n",
        "plt.xlabel(\"Average Salary (in K)\", fontsize=12)\n",
        "plt.ylabel(\"State\", fontsize=12)\n",
        "plt.grid(axis='x', linestyle=\"--\", alpha=0.3)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected a bar chart because it is the best visualization for comparing the salary levels across different locations (categorical variable).\n",
        "Location is one of the strongest determinants of salary, and understanding how compensation varies by state helps:\n",
        "\n",
        "Identify top-paying regions\n",
        "\n",
        "Understand cost-of-living salary adjustments\n",
        "\n",
        "Support business expansion planning\n",
        "\n",
        "Guide job seekers toward high-paying regions\n",
        "\n",
        "A bar chart clearly shows the top 10 states with highest salaries, making it ideal for comparative analysis and storytelling."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key insights from this chart:\n",
        "\n",
        "States like California (CA), New York (NY), Washington (WA), and Massachusetts (MA) consistently appear as the highest-paying locations.\n",
        "\n",
        "These states host major tech hubs such as Silicon Valley, Seattle, and New York City, which explains the higher compensation.\n",
        "\n",
        "Salaries in these states are significantly above the national average.\n",
        "\n",
        "Some states offer substantially lower pay, showing uneven distribution of tech salary opportunities across the U.S.\n",
        "\n",
        "This confirms that location plays a major role in determining salary due to factors such as:\n",
        "\n",
        "Tech cluster density\n",
        "\n",
        "Living costs\n",
        "\n",
        "Competition for skilled talent\n",
        "\n",
        "Presence of major tech companies (Google, Amazon, Meta, etc.)"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ” Positive Business Impact\n",
        "\n",
        "Yes, these insights can create major positive business impact:\n",
        "\n",
        "Job Seekers:\n",
        "Can target high-paying states for better career growth and salary negotiation.\n",
        "\n",
        "Employers & HR Teams:\n",
        "Helps in designing location-based salary brackets to stay competitive.\n",
        "\n",
        "Companies Planning Expansion:\n",
        "Can identify regions where salaries are high (competitive market) vs regions with lower hiring costs.\n",
        "\n",
        "Recruiters:\n",
        "Can adjust job postings and compensation packages based on location trends.\n",
        "\n",
        "â— Negative Growth Impact (If insights are ignored)\n",
        "\n",
        "If companies ignore location-based salary differences:\n",
        "\n",
        "They may underpay in high-cost states, leading to:\n",
        "\n",
        "Failed hiring\n",
        "\n",
        "Poor retention\n",
        "\n",
        "Loss of top talent\n",
        "\n",
        "They may overpay in low-cost states, leading to:\n",
        "\n",
        "Budget inefficiencies\n",
        "\n",
        "Lower ROI on hiring\n",
        "\n",
        "For job seekers, ignoring location trends may lead to:\n",
        "\n",
        "Incorrect salary expectations\n",
        "\n",
        "Lower negotiation confidence\n",
        "\n",
        "Missed opportunities in high-paying states\n",
        "\n",
        "Hence, analyzing salary by location is crucial for both job seekers and employers."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart â€“ 6: Relationship Between Company Rating and Average Salary"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 : Relationship Between Company Rating and Salary\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "sns.scatterplot(data=df, x='rating', y='avg_salary', alpha=0.6, color='teal')\n",
        "\n",
        "plt.title(\"Scatter Plot: Company Rating vs Average Salary\", fontsize=14)\n",
        "plt.xlabel(\"Company Rating\", fontsize=12)\n",
        "plt.ylabel(\"Average Salary (in K)\", fontsize=12)\n",
        "plt.grid(linestyle=\"--\", alpha=0.3)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected a scatter plot because both variables â€” rating and avg_salary â€” are numerical.\n",
        "A scatter plot is ideal for:\n",
        "\n",
        "Identifying correlation (positive, negative, or no relationship)\n",
        "\n",
        "Observing trends and patterns\n",
        "\n",
        "Spotting clusters of data points\n",
        "\n",
        "Detecting unusual combinations (high salary + low rating or vice versa)\n",
        "\n",
        "Company rating is often linked to company culture, financial stability, and employee satisfaction.\n",
        "We want to verify whether higher-rated companies also tend to offer higher salaries.\n",
        "\n",
        "Therefore, the scatter plot is the best visualization for this analysis."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key insights from the scatter plot:\n",
        "\n",
        "There is a mild positive correlation between company rating and salary.\n",
        "Higher-rated companies tend to offer slightly higher salaries on average.\n",
        "\n",
        "Companies with ratings above 3.5 frequently appear in the higher salary range.\n",
        "\n",
        "However, the pattern is not very strong, suggesting:\n",
        "\n",
        "Rating is not the strongest determining factor of salary\n",
        "\n",
        "Other factors (location, job title, company size) have stronger impact\n",
        "\n",
        "There are companies with high ratings but moderate salaries, indicating:\n",
        "\n",
        "Strong culture or employee satisfaction\n",
        "\n",
        "But limited budget or smaller company size\n",
        "\n",
        "There are also companies with low ratings but high salaries, which is common in:\n",
        "\n",
        "Companies with poor work culture offering higher salaries to attract talent"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ” Positive Business Impact\n",
        "\n",
        "Yes, this insight provides meaningful business value:\n",
        "\n",
        "Job Seekers:\n",
        "Can understand whether high ratings truly reflect better pay.\n",
        "Helps filter companies based on both compensation and work quality.\n",
        "\n",
        "HR & Employers:\n",
        "Helps companies with high ratings but low salaries reconsider salary competitiveness.\n",
        "\n",
        "Recruiters:\n",
        "Can create realistic job postings that align with company reputation.\n",
        "\n",
        "Workplace Analysts:\n",
        "Better understand the relationship between satisfaction and compensation.\n",
        "\n",
        "â— Negative Growth Impact (If insights are ignored)\n",
        "\n",
        "Ignoring these insights can cause:\n",
        "\n",
        "High-rated companies that underpay may lose skilled employees due to market mismatch.\n",
        "\n",
        "Low-rated companies that overpay may experience:\n",
        "\n",
        "Overspending on salaries\n",
        "\n",
        "Budget imbalance\n",
        "\n",
        "Higher employee turnover if compensation is the only attraction\n",
        "\n",
        "For job seekers:\n",
        "Assuming high ratings always mean high pay can lead to poor negotiation and wrong job choices.\n",
        "\n",
        "Thus, understanding this relationship prevents wrong salary expectations and improves hiring strategies."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart â€“ 7: Relationship Between Company Age and Average Salary"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 : Relationship Between Company Age and Average Salary\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "sns.scatterplot(\n",
        "    data=df,\n",
        "    x='company_age',\n",
        "    y='avg_salary',\n",
        "    alpha=0.6,\n",
        "    color='darkgreen'\n",
        ")\n",
        "\n",
        "plt.title(\"Scatter Plot: Company Age vs Average Salary\", fontsize=14)\n",
        "plt.xlabel(\"Company Age (Years)\", fontsize=12)\n",
        "plt.ylabel(\"Average Salary (in K)\", fontsize=12)\n",
        "plt.grid(linestyle=\"--\", alpha=0.3)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected a scatter plot because both variables â€” company_age and avg_salary â€” are numerical.\n",
        "This chart helps analyze the relationship between how old a company is and the salary it offers.\n",
        "\n",
        "A scatter plot is ideal because:\n",
        "\n",
        "It shows whether older companies pay more (positive trend)\n",
        "\n",
        "It shows whether newer startups offer higher salaries (negative trend)\n",
        "\n",
        "Helps identify clusters (e.g., very old companies with low salaries, young startups with high salaries)\n",
        "\n",
        "Reveals non-linear patterns\n",
        "\n",
        "Understanding how company age influences salary is important for job seekers and employers, making a scatter plot the best analytical tool."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key insights from the scatter plot:\n",
        "\n",
        "There is a mild positive correlation:\n",
        "Older companies tend to offer slightly higher salaries, but not always.\n",
        "\n",
        "Many younger companies (less than 10 years old) offer competitive or higher salaries, which aligns with:\n",
        "\n",
        "Startups paying more to attract skilled talent\n",
        "\n",
        "Fast-growing tech companies overpaying to remain competitive\n",
        "\n",
        "Older companies (30â€“100 years old) form a wide salary distribution, indicating:\n",
        "\n",
        "Stable organizations\n",
        "\n",
        "Structured pay scales\n",
        "\n",
        "Salary differences across industries\n",
        "\n",
        "No perfect linear trend exists â€” salary is influenced by many factors (location, job title, size), not just company age."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ” Positive Business Impact\n",
        "\n",
        "Yes, the insights are very useful:\n",
        "\n",
        "Job Seekers:\n",
        "Understand that both new and old companies can offer high salaries; age alone should not determine job decisions.\n",
        "\n",
        "HR & Employers:\n",
        "Young companies that do not offer competitive salaries may lose talent to better-paying startups.\n",
        "Older companies can benchmark their pay to stay attractive in a competitive market.\n",
        "\n",
        "Investors / Analysts:\n",
        "Can analyze whether young companies compensate aggressively to attract talent and grow quickly.\n",
        "\n",
        "Recruiters:\n",
        "Can design competitive salary packages based on company maturity.\n",
        "\n",
        "â— Negative Growth Impact (If insights are ignored)\n",
        "\n",
        "Ignoring these insights may cause:\n",
        "\n",
        "New companies to underpay and fail to attract skilled professionals.\n",
        "\n",
        "Old companies to assume brand value is enough, leading to talent loss.\n",
        "\n",
        "ML models may mispredict salary if company age is not used as a feature.\n",
        "\n",
        "Job seekers may wrongly assume older companies always pay more or that young startups always underpay.\n",
        "\n",
        "Understanding the balance helps avoid poor hiring and career decisions."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart â€“ 8: Average Salary by Company Revenue Category"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 : Average Salary by Company Revenue Category\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12,7))\n",
        "\n",
        "# Compute average salary for each revenue category\n",
        "if 'revenue' in df.columns:\n",
        "    revenue_salary = (df.groupby('revenue')['avg_salary']\n",
        "                      .mean()\n",
        "                      .sort_values(ascending=False))\n",
        "\n",
        "    sns.barplot(\n",
        "        x=revenue_salary.values,\n",
        "        y=revenue_salary.index,\n",
        "        palette=\"Spectral\"\n",
        "    )\n",
        "\n",
        "    plt.title(\"Average Salary by Company Revenue Category\", fontsize=14)\n",
        "    plt.xlabel(\"Average Salary (in K)\", fontsize=12)\n",
        "    plt.ylabel(\"Revenue Category\", fontsize=12)\n",
        "    plt.grid(axis='x', linestyle=\"--\", alpha=0.3)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Column 'revenue' not found in dataset.\")\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected a bar chart because revenue is a categorical variable, and we want to compare how the average salary varies across different revenue categories.\n",
        "\n",
        "Revenue directly reflects a companyâ€™s:\n",
        "\n",
        "Financial stability\n",
        "\n",
        "Cash flow\n",
        "\n",
        "Hiring capacity\n",
        "\n",
        "Ability to pay competitive salaries\n",
        "\n",
        "A bar chart clearly visualizes:\n",
        "\n",
        "Which revenue categories offer the highest compensation\n",
        "\n",
        "Whether larger revenue translates to higher pay\n",
        "\n",
        "Differences between small, medium, and large revenue companies\n",
        "\n",
        "This makes it the most suitable chart for understanding salary based on company financial strength."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key insights from the chart:\n",
        "\n",
        "Companies with >$10B annual revenue offer the highest salaries.\n",
        "\n",
        "Companies in the $1Bâ€“$10B category also pay competitively.\n",
        "\n",
        "Mid-revenue companies ($100Mâ€“$1B) offer moderate salaries.\n",
        "\n",
        "Very small companies (<$1M or $1Mâ€“$10M) tend to offer significantly lower salaries.\n",
        "\n",
        "This indicates a clear positive correlation between company revenue and employee compensation.\n",
        "\n",
        "Why?\n",
        "Because high-revenue companies:\n",
        "\n",
        "Have larger hiring budgets\n",
        "\n",
        "Can afford specialized tech talent\n",
        "\n",
        "Operate in competitive markets\n",
        "\n",
        "Provide structured salary bands\n",
        "\n",
        "Lower-revenue companies may struggle to offer competitive pay due to limited resources."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ” Positive Business Impact\n",
        "\n",
        "Yes, these insights provide clear value:\n",
        "\n",
        "Job Seekers:\n",
        "Can prioritize high-revenue companies for better salary expectations.\n",
        "\n",
        "Startups & Small Companies:\n",
        "Understand the need to offer equity, bonuses, or flexible perks to compete with high-revenue companies.\n",
        "\n",
        "Employers:\n",
        "Benchmark compensation based on revenue peers.\n",
        "\n",
        "Recruiters:\n",
        "Set realistic expectations when sourcing talent for smaller companies.\n",
        "\n",
        "Business Analysts:\n",
        "Identify how financial strength is linked to talent acquisition success.\n",
        "\n",
        "â— Negative Growth Impact (If insights are ignored)\n",
        "\n",
        "If businesses ignore revenue-based salary trends:\n",
        "\n",
        "Small companies may continuously lose talent to large companies.\n",
        "\n",
        "Low revenue companies may experience:\n",
        "\n",
        "High employee turnover\n",
        "\n",
        "Difficult hiring processes\n",
        "\n",
        "Candidates may undervalue higher-paying opportunities due to lack of market awareness.\n",
        "\n",
        "ML models may mispredict salaries without revenue as a key feature.\n",
        "\n",
        "Ignoring salary vs revenue relationships leads to poor recruitment strategy and budgeting errors."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart â€“ 9: Average Salary by Industry (Top 10 Industries)"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 : Average Salary by Industry (Top 10)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12,7))\n",
        "\n",
        "# Ensure 'industry' exists\n",
        "if 'industry' in df.columns:\n",
        "    # Compute top 10 highest paying industries\n",
        "    top_industries = (df.groupby('industry')['avg_salary']\n",
        "                        .mean()\n",
        "                        .sort_values(ascending=False)\n",
        "                        .head(10))\n",
        "\n",
        "    sns.barplot(\n",
        "        x=top_industries.values,\n",
        "        y=top_industries.index,\n",
        "        palette=\"viridis\"\n",
        "    )\n",
        "\n",
        "    plt.title(\"Top 10 Highest Paying Industries (Avg Salary)\", fontsize=14)\n",
        "    plt.xlabel(\"Average Salary (in K)\", fontsize=12)\n",
        "    plt.ylabel(\"Industry\", fontsize=12)\n",
        "    plt.grid(axis='x', linestyle=\"--\", alpha=0.3)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Column 'industry' not found in dataset.\")\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a bar chart because it clearly compares how salaries vary across different industry categories, which are categorical variables.\n",
        "\n",
        "Industry plays a major role in determining salary because:\n",
        "\n",
        "Some industries (like Tech, AI, Software, Cloud, Cybersecurity) offer significantly higher salaries.\n",
        "\n",
        "Others (like Education, Non-Profit, Healthcare Administration) typically offer lower salary bands.\n",
        "\n",
        "A bar chart helps visualize the top 10 highest paying industries, making it useful for:\n",
        "\n",
        "Market analysis\n",
        "\n",
        "Salary benchmarking\n",
        "\n",
        "Career planning\n",
        "\n",
        "Business strategy\n",
        "\n",
        "Hence, this chart is both analytically powerful and easy to interpret."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possible key insights from the chart:\n",
        "\n",
        "Internet & Software, Computer Hardware, IT Services, Enterprise Software, and Cloud Computing industries offer the highest average salaries.\n",
        "\n",
        "Industries related to AI, Data Science, and Cybersecurity appear in the top-paying sectors consistently.\n",
        "\n",
        "Traditional industries such as Retail, Education, or Manufacturing typically fall near the bottom.\n",
        "\n",
        "Salary differences across industries can be very large, indicating strong industry-wise pay segmentation.\n",
        "\n",
        "High-paying industries are often technology-driven and operate in highly competitive talent markets.\n",
        "\n",
        "These insights confirm that the industry has a major influence on salary levels, independent of job title or location."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ” Positive Business Impact\n",
        "\n",
        "This insight is highly valuable for all stakeholders:\n",
        "\n",
        "Job Seekers:\n",
        "Can target high-paying industries to maximize earning potential and career growth.\n",
        "\n",
        "Companies / HR Teams:\n",
        "Can compare their salary standards with other industries and adjust pay scales.\n",
        "\n",
        "Recruiters:\n",
        "Can focus hiring strategies on industries where competition is high and salaries need to be competitive.\n",
        "\n",
        "Business Analysts:\n",
        "Helps forecast salary trends and identify high-growth industries.\n",
        "\n",
        "â— Negative Growth Impact (If insights are ignored)\n",
        "\n",
        "Ignoring industry-wise salary differences can lead to:\n",
        "\n",
        "Employees joining lower-paying industries without understanding salary caps.\n",
        "\n",
        "Companies in high-paying industries offering below-market salaries, leading to:\n",
        "\n",
        "Attrition\n",
        "\n",
        "Poor hiring\n",
        "\n",
        "Difficulty attracting engineers\n",
        "\n",
        "A mismatch in compensation strategy, leading to financial inefficiencies.\n",
        "\n",
        "ML models underperforming if industry is not included as a feature.\n",
        "\n",
        "Thus, salary vs industry insight is a crucial factor in compensation planning and prediction."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart â€“ 10: Average Salary by Sector (Top 10 Sectors)"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 : Average Salary by Sector (Top 10)\n",
        "\n",
        "plt.figure(figsize=(12,7))\n",
        "\n",
        "# Ensure 'sector' exists\n",
        "if 'sector' in df.columns:\n",
        "    # Compute top 10 highest paying sectors\n",
        "    top_sectors = (df.groupby('sector')['avg_salary']\n",
        "                      .mean()\n",
        "                      .sort_values(ascending=False)\n",
        "                      .head(10))\n",
        "\n",
        "    sns.barplot(\n",
        "        x=top_sectors.values,\n",
        "        y=top_sectors.index,\n",
        "        palette=\"coolwarm\"\n",
        "    )\n",
        "\n",
        "    plt.title(\"Top 10 Highest Paying Sectors (Avg Salary)\", fontsize=14)\n",
        "    plt.xlabel(\"Average Salary (in K)\", fontsize=12)\n",
        "    plt.ylabel(\"Sector\", fontsize=12)\n",
        "    plt.grid(axis='x', linestyle=\"--\", alpha=0.3)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Column 'sector' not found in dataset.\")\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected a bar chart because the â€œsectorâ€ variable is categorical, and we want to compare how salaries vary across different sectors.\n",
        "Sector is broader than industry and helps capture high-level market trends.\n",
        "\n",
        "This chart is ideal because:\n",
        "\n",
        "It highlights the top-paying sectors.\n",
        "\n",
        "Provides a clear comparison across categories.\n",
        "\n",
        "Helps identify which sectors invest more in tech talent.\n",
        "\n",
        "Is easy to read and interpret for stakeholders.\n",
        "\n",
        "A bar chart is therefore the best visualization for showing sector-wise salary differences."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key insights from the visualization:\n",
        "\n",
        "The Technology, Aerospace & Defense, Biotech/Pharmaceutical, and Financial Services sectors tend to offer the highest average salaries.\n",
        "\n",
        "Sectors like Education, Retail, Hospitality, and Non-profit typically offer lower compensation.\n",
        "\n",
        "High-paying sectors are often:\n",
        "\n",
        "Innovation-driven\n",
        "\n",
        "Capital-intensive\n",
        "\n",
        "Competitive\n",
        "\n",
        "Dependent on highly skilled labor\n",
        "\n",
        "Sectors with lower salaries usually operate under:\n",
        "\n",
        "Limited profit margins\n",
        "\n",
        "Less dependency on niche technical roles\n",
        "\n",
        "Cost-sensitive business models\n",
        "\n",
        "This confirms that sector significantly influences salary level, independent of job title."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ” Positive Business Impact\n",
        "\n",
        "Yes, the insights provide significant business value:\n",
        "\n",
        "Job Seekers:\n",
        "Can choose high-paying sectors for better long-term earning potential.\n",
        "\n",
        "HR Teams:\n",
        "Can adjust salary structures to remain competitive within their sector.\n",
        "\n",
        "Recruiters:\n",
        "Can understand sector-specific salary benchmarks to create accurate job postings.\n",
        "\n",
        "Companies:\n",
        "Gain an understanding of how competitive their compensation is compared to sector peers.\n",
        "\n",
        "Business Strategy:\n",
        "Helps allocate budget to roles within high-paying sectors to attract top talent.\n",
        "\n",
        "â— Negative Growth Impact (If insights are ignored)\n",
        "\n",
        "Failure to use these insights may lead to:\n",
        "\n",
        "Underpaying technical talent in high-paying sectors â†’ talent loss and high turnover.\n",
        "\n",
        "Overpaying in low-paying sectors â†’ unnecessary financial burden.\n",
        "\n",
        "Candidates entering low-paying sectors without understanding salary limitations.\n",
        "\n",
        "ML model performance decreases if sector is ignored, leading to inaccurate salary predictions.\n",
        "\n",
        "Ignoring sector-wise salary trends can therefore create major hiring and business challenges."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart â€“ 11: Relationship Between Job Description Length and Average Salary"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 : Salary vs Job Description Length\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "sns.scatterplot(\n",
        "    data=df,\n",
        "    x='sdesc_len',\n",
        "    y='avg_salary',\n",
        "    alpha=0.6,\n",
        "    color='purple'\n",
        ")\n",
        "\n",
        "plt.title(\"Scatter Plot: Job Description Length vs Average Salary\", fontsize=14)\n",
        "plt.xlabel(\"Job Description Length (characters)\", fontsize=12)\n",
        "plt.ylabel(\"Average Salary (in K)\", fontsize=12)\n",
        "plt.grid(linestyle=\"--\", alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a scatter plot because both variables â€” job description length (sdesc_len) and salary (avg_salary) â€” are numerical.\n",
        "A scatter plot helps:\n",
        "\n",
        "Identify whether longer job descriptions correlate with higher salaries\n",
        "\n",
        "Spot patterns in job complexity vs compensation\n",
        "\n",
        "Detect clusters of high or low salary roles\n",
        "\n",
        "Determine if this engineered feature is important for ML models\n",
        "\n",
        "This feature is often overlooked but can carry meaningful insights about the complexity and seniority of roles.\n",
        "\n",
        "Thus, a scatter plot is the best visualization for this analysis."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possible insights from the visualization:\n",
        "\n",
        "There is a slight upward trend:\n",
        "Jobs with longer descriptions tend to offer higher salaries, indicating that more complex roles require more skills and therefore pay better.\n",
        "\n",
        "Many short job descriptions are associated with lower salary ranges, usually junior or entry-level roles.\n",
        "\n",
        "Jobs with extremely long descriptions (1000+ characters) often correspond to senior, specialized, or multifaceted roles, which explains the higher pay.\n",
        "\n",
        "The spread of points suggests that description length alone cannot fully predict salary, but it does contribute to understanding role complexity.\n",
        "\n",
        "There are also some jobs with long descriptions but moderate salaries, which may indicate:\n",
        "\n",
        "Mid-sized companies with detailed job postings\n",
        "\n",
        "Strict skill requirements but lower budgets"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ” Positive Business Impact\n",
        "\n",
        "Yes, these insights are useful:\n",
        "\n",
        "Job Seekers:\n",
        "Can estimate the complexity of a job based on description length and expect corresponding salary ranges.\n",
        "\n",
        "Recruiters:\n",
        "Understand that overly short job postings may fail to attract high-quality applicants, as they usually align with lower salaries.\n",
        "\n",
        "Companies:\n",
        "Can benchmark job description standards â€” ensuring detailed descriptions for high-skill roles to attract top talent.\n",
        "\n",
        "ML Modeling:\n",
        "sdesc_len becomes a meaningful engineered feature that improves model accuracy.\n",
        "\n",
        "â— Negative Growth Impact (If insights are ignored)\n",
        "\n",
        "Ignoring these insights may lead to:\n",
        "\n",
        "Poor hiring quality due to unclear or short job descriptions for complex roles.\n",
        "\n",
        "Job seekers misjudging the complexity or salary potential of roles.\n",
        "\n",
        "ML models losing predictive power if sdesc_len is not included, reducing salary prediction accuracy.\n",
        "\n",
        "Companies putting minimal effort into job descriptions, reducing their credibility and ability to attract skilled talent.\n",
        "\n",
        "Thus, job description quality plays a subtle but important role in salary expectations and hiring outcomes."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart â€“ 12: Correlation Heatmap of Numerical Variables"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 : Correlation Heatmap of Numerical Variables\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "\n",
        "# Select only numerical columns\n",
        "num_df = df.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = num_df.corr()\n",
        "\n",
        "# Heatmap\n",
        "sns.heatmap(\n",
        "    corr_matrix,\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    cmap=\"coolwarm\",\n",
        "    linewidths=0.5\n",
        ")\n",
        "\n",
        "plt.title(\"Correlation Heatmap of Numerical Features\", fontsize=14)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected a correlation heatmap because it is the most effective visualization for understanding multivariate relationships between numerical variables.\n",
        "\n",
        "This chart helps:\n",
        "\n",
        "Identify which variables have strong positive or negative correlations\n",
        "\n",
        "Detect features that might be useful predictors in machine learning models\n",
        "\n",
        "Avoid multicollinearity by identifying highly correlated pairs\n",
        "\n",
        "Understand overall data structure at a glance\n",
        "\n",
        "Since salary prediction is a regression problem, it is essential to understand how features correlate with the target (avg_salary) and with each other.\n",
        "\n",
        "A heatmap provides a holistic, easy-to-read overview of all numerical interactions in the dataset."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key insights from the correlation heatmap:\n",
        "\n",
        "avg_salary has the strongest correlations with features like:\n",
        "\n",
        "company_age (mild positive correlation)\n",
        "\n",
        "rating (slight positive correlation)\n",
        "\n",
        "num_comp (varies depending on dataset, but often positive)\n",
        "\n",
        "Some numerical features show weak or near-zero correlation, indicating:\n",
        "\n",
        "They may still be useful in ML models (nonlinear models like Random Forest)\n",
        "\n",
        "But linear models may not benefit much from them\n",
        "\n",
        "There is no major multicollinearity between numerical variables (correlations above 0.80), which is good for model stability.\n",
        "\n",
        "Features like:\n",
        "\n",
        "hourly_min, hourly_max (if present)\n",
        "strongly correlate with each other â€” which makes sense since both represent hourly salary ranges.\n",
        "\n",
        "Job description length (sdesc_len) usually has low but positive correlation with salary, confirming that complex jobs tend to pay slightly more.\n",
        "\n",
        "Overall, the correlation matrix reveals that salary prediction requires both numerical and categorical features, because no single numeric feature is a very strong predictor."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ” Positive Business Impact\n",
        "\n",
        "The insights from this heatmap are extremely valuable:\n",
        "\n",
        "For Data Scientists:\n",
        "Helps with feature selection and engineering for improved ML model performance.\n",
        "\n",
        "For Businesses / HR Teams:\n",
        "Understanding which factors correlate with salary helps improve:\n",
        "\n",
        "Hiring strategies\n",
        "\n",
        "Salary benchmarking\n",
        "\n",
        "Competitive compensation planning\n",
        "\n",
        "For Job Seekers:\n",
        "Insights into what influences salary (rating, company age, complexity of roles) help guide career decisions.\n",
        "\n",
        "For ML Models:\n",
        "Helps avoid multicollinearity and optimize feature importance.\n",
        "\n",
        "â— Negative Growth Impact (If insights are ignored)\n",
        "\n",
        "Ignoring correlation insights can lead to:\n",
        "\n",
        "Poor ML Model Performance:\n",
        "Using redundant or weak features reduces accuracy and interpretability.\n",
        "\n",
        "Businesses Misjudging Salary Drivers:\n",
        "Leads to wrong compensation strategies â†’ talent loss.\n",
        "\n",
        "Incorrect Decision-Making:\n",
        "If companies assume certain features strongly influence salary without checking correlations, they may overpay or underpay roles.\n",
        "\n",
        "Job Seekers Misinterpreting Patterns:\n",
        "Wrong assumptions about what features impact salary can lead to ineffective career planning.\n",
        "\n",
        "Thus, correlation analysis is critical for both business and data science decisions."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart â€“ 13: Pairplot of Key Numerical Features"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 : Pairplot of Key Numerical Features\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(10,8))\n",
        "\n",
        "# Select meaningful numeric features for multivariate exploration\n",
        "pairplot_cols = ['avg_salary', 'rating', 'company_age', 'sdesc_len']\n",
        "\n",
        "sns.pairplot(\n",
        "    df[pairplot_cols],\n",
        "    diag_kind='kde',\n",
        "    corner=True,\n",
        "    plot_kws={'alpha': 0.6}\n",
        ")\n",
        "\n",
        "plt.suptitle(\"Pairplot of Key Numerical Variables\", y=1.02, fontsize=14)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected a pairplot because it allows simultaneous visualization of multiple numerical relationships in the dataset.\n",
        "A pairplot is useful because:\n",
        "\n",
        "It shows pairwise scatter plots for each feature combination.\n",
        "\n",
        "It shows distribution curves (KDE) for each individual variable.\n",
        "\n",
        "It reveals hidden patterns that simple univariate or bivariate charts cannot show.\n",
        "\n",
        "It helps check whether relationships are linear, nonlinear, or random.\n",
        "\n",
        "It highlights correlations visually through slope and clustering.\n",
        "\n",
        "Since this is a salary prediction project, understanding multivariate numerical interactions is essential for feature selection and model development."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key observations from the pairplot:\n",
        "\n",
        "avg_salary vs rating:\n",
        "Shows a mild positive trend, supporting earlier findings that higher-rated companies tend to pay slightly more.\n",
        "\n",
        "avg_salary vs company_age:\n",
        "Displays a weak upward slope, suggesting that older companies often pay higher salaries.\n",
        "\n",
        "avg_salary vs sdesc_len:\n",
        "Indicates that jobs with longer descriptions generally correspond to roles with higher pay.\n",
        "\n",
        "rating vs company_age:\n",
        "Shows scattered distribution, meaning company age is not strongly linked to rating quality.\n",
        "\n",
        "Univariate KDE plots show that:\n",
        "\n",
        "Salary is right-skewed\n",
        "\n",
        "sdesc_len has a wide variance\n",
        "\n",
        "Rating is mostly clustered around 3.0â€“4.0\n",
        "\n",
        "Company age has two common ranges: young companies and very old companies\n",
        "\n",
        "Overall, the pairplot confirms earlier findings:\n",
        "Salary depends on multiple features, not just one, and the relationships are mostly mild-to-moderate, not strong linear relationships."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ” Positive Business Impact\n",
        "\n",
        "Yes, these insights greatly benefit various stakeholders:\n",
        "\n",
        "Data Scientists:\n",
        "Understand which features show linear or nonlinear patterns, helping choose suitable algorithms (tree-based models work better with nonlinear patterns).\n",
        "\n",
        "HR Teams & Companies:\n",
        "Recognize that salary is influenced by several factors (rating, company age, description complexity), not just job title or location.\n",
        "\n",
        "Recruiters:\n",
        "Use this insight to adjust job descriptions and hiring strategies.\n",
        "\n",
        "Job Seekers:\n",
        "Learn that salary depends on many interconnected variables, helping set realistic expectations.\n",
        "\n",
        "â— Negative Growth Impact (If insights are ignored)\n",
        "\n",
        "Ignoring these insights may lead to:\n",
        "\n",
        "Incorrect ML model selection, reducing salary prediction accuracy.\n",
        "\n",
        "Companies overlooking factors like job complexity may offer incorrect salary packages.\n",
        "\n",
        "Overemphasis on a single feature (like rating) may lead to flawed decisions.\n",
        "\n",
        "Job seekers misunderstanding what actually drives salary differences.\n",
        "\n",
        "Thus, multivariate analysis helps prevent oversimplification and incorrect assumptions."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 14 : Extended Correlation Heatmap\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "\n",
        "# Select important numerical features\n",
        "num_cols = ['avg_salary', 'rating', 'company_age', 'sdesc_len']\n",
        "\n",
        "# If encoded columns exist (after OneHotEncoding), you may extend here\n",
        "# For now, focus on core numerical variables\n",
        "corr_matrix_extended = df[num_cols].corr()\n",
        "\n",
        "sns.heatmap(\n",
        "    corr_matrix_extended,\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    cmap='PuBuGn',\n",
        "    linewidths=0.5\n",
        ")\n",
        "\n",
        "plt.title(\"Correlation Heatmap of Key Numerical Variables\", fontsize=14)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected a correlation heatmap because it is the most effective visualization to understand how multiple numerical features relate to each other simultaneously, especially when preparing data for machine learning models.\n",
        "\n",
        "This heatmap is different from earlier ones because:\n",
        "\n",
        "It focuses only on core predictive numerical variables\n",
        "\n",
        "Offers a clearer multivariate view without clutter\n",
        "\n",
        "Helps identify potential feature interactions\n",
        "\n",
        "Helps remove redundant or weak features\n",
        "\n",
        "Assists in deciding which features will actually improve model performance\n",
        "\n",
        "A heatmap gives a comprehensive summary of the strength and direction of relationships between variables, which is critical for regression modeling."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key insights from this extended correlation heatmap:\n",
        "\n",
        "âœ” avg_salary correlations\n",
        "\n",
        "avg_salary has mild positive correlation with rating, meaning higher-rated companies pay slightly more.\n",
        "\n",
        "It also has a weak but noticeable correlation with company_age, indicating older companies offer moderately higher salaries.\n",
        "\n",
        "sdesc_len shows a small positive correlation with salary â€” longer, more detailed job descriptions typically represent higher-skilled roles.\n",
        "\n",
        "âœ” Low multicollinearity\n",
        "\n",
        "The numerical variables do not show high correlation among themselves, which means:\n",
        "\n",
        "No multicollinearity problems\n",
        "\n",
        "All features can be used in ML models\n",
        "\n",
        "Both linear and nonlinear models will perform well\n",
        "\n",
        "âœ” Each variable adds unique information\n",
        "\n",
        "Since correlations are not too high, each feature contributes different predictive value.\n",
        "\n",
        "This improves model interpretability and performance."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 15 : Pair Plot of Key Numerical Features\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(10,8))\n",
        "\n",
        "# Select the most relevant numerical features for pairwise analysis\n",
        "pairplot_features = ['avg_salary', 'rating', 'company_age', 'sdesc_len']\n",
        "\n",
        "sns.pairplot(\n",
        "    df[pairplot_features],\n",
        "    diag_kind='kde',       # smooth KDE curve for diagonal plots\n",
        "    corner=True,           # show only lower triangle for clarity\n",
        "    plot_kws={'alpha': 0.6, 'color': 'navy'}\n",
        ")\n",
        "\n",
        "plt.suptitle(\"Pair Plot of Numerical Variables\", y=1.02, fontsize=14)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected a Pair Plot because it provides a comprehensive multivariate view of how multiple numerical variables interact with each other.\n",
        "Unlike individual scatter plots, a pair plot:\n",
        "\n",
        "Shows pairwise relationships between all selected variables\n",
        "\n",
        "Reveals patterns, clusters, and trends\n",
        "\n",
        "Includes distribution plots (KDE) to understand the spread of each variable\n",
        "\n",
        "Helps determine whether relationships are linear, nonlinear, or random\n",
        "\n",
        "Assists in selecting the best features for machine learning models\n",
        "\n",
        "A pair plot is extremely useful for understanding the dataset holistically and making informed decisions during feature engineering and modeling."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the pair plot, we can see the following insights:\n",
        "\n",
        "âœ” avg_salary vs rating\n",
        "\n",
        "A mild positive trend indicates that companies with higher ratings tend to offer slightly better salaries.\n",
        "\n",
        "âœ” avg_salary vs company_age\n",
        "\n",
        "A weak upward trend suggests older companies often pay moderately higher salaries, but the relationship is not very strong.\n",
        "\n",
        "âœ” avg_salary vs sdesc_len\n",
        "\n",
        "A slightly increasing pattern indicates jobs with longer descriptions tend to have higher salaries, meaning more complex roles offer higher compensation.\n",
        "\n",
        "âœ” rating vs company_age\n",
        "\n",
        "Very scattered â€” no strong relationship.\n",
        "Company age does not necessarily impact company rating.\n",
        "\n",
        "âœ” KDE distributions\n",
        "\n",
        "avg_salary is right-skewed.\n",
        "\n",
        "rating is mostly clustered around 3â€“4.5.\n",
        "\n",
        "company_age has a wide spread showing a mix of young and very old companies.\n",
        "\n",
        "sdesc_len shows high variability in job complexity.\n",
        "\n",
        "âœ” Overall insight\n",
        "\n",
        "No single feature strongly predicts salary by itself, meaning salary depends on multiple interacting factors, validating the need for multivariate ML models (e.g., Random Forest, XGBoost)."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on your chart experiments, here are three hypothetical statements:\n",
        "Hypothesis 1:\n",
        "\n",
        "Companies with higher ratings offer higher average salaries.\n",
        "\n",
        "Hypothesis 2:\n",
        "\n",
        "Tech-heavy job titles (Data Scientist, ML Engineer) have significantly higher salaries than non-tech/general roles.\n",
        "\n",
        "Hypothesis 3:\n",
        "\n",
        "Salaries differ significantly across different company sizes."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1 : Companies with higher Glassdoor ratings offer higher salaries."
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (Hâ‚€):\n",
        "\n",
        "There is no significant difference in the average salary of companies with high ratings vs low ratings.\n",
        "(Company rating does not impact salary.)\n",
        "\n",
        "Alternate Hypothesis (Hâ‚):\n",
        "\n",
        "There is a significant difference in the average salary between high-rated and low-rated companies.\n",
        "(Company rating does impact salary.)"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hypothesis Test 1: Do higher-rated companies offer higher salaries?\n",
        "# Independent Two-Sample t-test\n",
        "\n",
        "\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Create two groups\n",
        "high_rated = df[df['rating'] >= 3.5]['avg_salary']\n",
        "low_rated  = df[df['rating'] <  3.5]['avg_salary']\n",
        "\n",
        "# Perform independent t-test\n",
        "t_stat, p_value = ttest_ind(high_rated, low_rated, equal_var=False)\n",
        "\n",
        "print(\"T-Statistic:\", t_stat)\n",
        "print(\"P-Value:\", p_value)\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I performed an Independent Two-Sample t-test (also known as an unpaired t-test) to obtain the P-value.Answer Here."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the Independent Two-Sample t-test because the hypothesis compares the mean salary between two independent groups:\n",
        "\n",
        "Group 1: High-rated companies (rating â‰¥ 3.5)\n",
        "\n",
        "Group 2: Low-rated companies (rating < 3.5)\n",
        "\n",
        "The t-test is the most appropriate statistical method because:\n",
        "\n",
        "The dependent variable (avg_salary) is numerical and continuous, which satisfies the t-test requirement.\n",
        "\n",
        "There are exactly two independent groups, so a two-sample test is suitable.\n",
        "\n",
        "The sample size is sufficiently large, making the t-test reliable even if data is not perfectly normal.\n",
        "\n",
        "The goal is to test whether the mean salaries differ significantly between the two groups â€” which is exactly what a two-sample t-test measures.\n",
        "\n",
        "The groups are independent of each other, meaning one company's rating doesnâ€™t impact another's.\n",
        "\n",
        "Therefore, an Independent Two-Sample t-test is the most appropriate and statistically valid method to evaluate whether company rating impacts salary."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2: Tech-heavy job titles (Data Scientist, ML Engineer) have significantly higher salaries than non-tech/general roles."
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (Hâ‚€):\n",
        "The average salary of tech-heavy roles and non-tech/general roles is the same.\n",
        "(There is no significant difference in mean salary between tech and non-tech roles.)\n",
        "\n",
        "Alternate Hypothesis (Hâ‚):\n",
        "The average salary of tech-heavy roles is significantly higher than non-tech/general roles.\n",
        "(There is a significant difference, and tech roles earn more.)"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hypothesis 2: Do tech-heavy roles earn higher salaries\n",
        "# than non-tech/general roles?\n",
        "# Statistical Test: Independent Two-Sample t-test\n",
        "\n",
        "\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Step 1: Define tech-heavy roles (in lowercase for matching)\n",
        "tech_roles = [\n",
        "    \"data scientist\",\n",
        "    \"machine learning engineer\",\n",
        "    \"ai engineer\",\n",
        "    \"data engineer\",\n",
        "    \"software engineer\"\n",
        "]\n",
        "\n",
        "# Step 2: Create a cleaned job title column for matching\n",
        "df['job_title_clean'] = df['job_title'].str.lower()\n",
        "\n",
        "# Step 3: Create a new categorical feature 'role_type'\n",
        "df['role_type'] = df['job_title_clean'].apply(\n",
        "    lambda x: \"Tech\" if any(role in x for role in tech_roles) else \"Non-Tech\"\n",
        ")\n",
        "\n",
        "# Step 4: Split salaries into two groups\n",
        "tech_salaries = df[df['role_type'] == \"Tech\"]['avg_salary']\n",
        "nontech_salaries = df[df['role_type'] == \"Non-Tech\"]['avg_salary']\n",
        "\n",
        "# Step 5: Perform Independent Two-Sample t-test\n",
        "t_stat2, p_value2 = ttest_ind(tech_salaries, nontech_salaries, equal_var=False)\n",
        "\n",
        "print(\"T-Statistic:\", t_stat2)\n",
        "print(\"P-Value:\", p_value2)\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have performed an Independent Two-Sample t-test (unpaired t-test) to obtain the P-value."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the Independent Two-Sample t-test because:\n",
        "\n",
        "The dependent variable (avg_salary) is numerical and continuous, which is required for a t-test.\n",
        "\n",
        "The comparison is between two independent groups:\n",
        "\n",
        "Tech-heavy job roles (Data Scientist, ML Engineer, etc.)\n",
        "\n",
        "Non-tech/general job roles\n",
        "\n",
        "The goal is to determine whether the mean salaries of the two groups differ significantly.\n",
        "\n",
        "Each observation belongs to one group only (independent samples).\n",
        "\n",
        "The sample size is sufficiently large, making the t-test robust and reliable, even if salary data is not perfectly normally distributed.\n",
        "\n",
        "Because of these reasons, the Independent Two-Sample t-test is the most appropriate statistical method for testing whether tech roles earn significantly higher salaries than non-tech roles."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3: Salaries differ significantly across different company sizes."
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis (Hâ‚€):**\n",
        "\n",
        "There is no significant difference in the mean salaries across different company size categories.\n",
        "(All company size groups have the same average salary.)\n",
        "\n",
        "**Alternate Hypothesis (Hâ‚):**\n",
        "\n",
        "There is a significant difference in mean salaries across company size categories.\n",
        "(At least one company size group has a different average salary.)"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hypothesis 3: Salaries differ across company size groups\n",
        "# Statistical Test: One-Way ANOVA\n",
        "\n",
        "\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Remove missing size values (if any)\n",
        "df_size = df.dropna(subset=['size'])\n",
        "\n",
        "# Group salaries by company size\n",
        "salary_groups = [group['avg_salary'].values\n",
        "                 for name, group in df_size.groupby('size')]\n",
        "\n",
        "# Perform One-Way ANOVA test\n",
        "f_stat, p_value3 = f_oneway(*salary_groups)\n",
        "\n",
        "print(\"F-Statistic:\", f_stat)\n",
        "print(\"P-Value:\", p_value3)\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I performed a One-Way ANOVA (Analysis of Variance) test to obtain the P-value."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the One-Way ANOVA because:\n",
        "\n",
        "The independent variable company size has more than two categories, such as:\n",
        "\n",
        "1â€“50 employees\n",
        "\n",
        "51â€“200 employees\n",
        "\n",
        "201â€“500 employees\n",
        "\n",
        "5001â€“10,000 employees\n",
        "\n",
        "10,001+ employees\n",
        "\n",
        "The dependent variable avg_salary is continuous and numeric, which is required for ANOVA.\n",
        "\n",
        "ANOVA is the appropriate test when comparing the mean values across 3 or more independent groups.\n",
        "\n",
        "The goal is to determine whether at least one groupâ€™s mean salary is significantly different from the others.\n",
        "\n",
        "Salaries from one company size group do not affect salaries from another, so the groups are independent, fulfilling the ANOVA requirement.\n",
        "\n",
        "Therefore, One-Way ANOVA is the correct statistical test to examine whether salary levels vary significantly across different company sizes."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# HANDLING MISSING VALUES & MISSING VALUE IMPUTATION\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Checking missing values initially\n",
        "print(\"Missing Values Before Imputation:\")\n",
        "print(df.isna().sum(), \"\\n\")\n",
        "\n",
        "# 1. Drop columns with very high missing percentage (>40%)\n",
        "missing_percentage = df.isna().mean() * 100\n",
        "cols_to_drop = missing_percentage[missing_percentage > 40].index.tolist()\n",
        "\n",
        "print(\"Columns dropped due to >40% missing values:\")\n",
        "print(cols_to_drop, \"\\n\")\n",
        "\n",
        "df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "# 2. Impute Numerical Columns using Median\n",
        "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "for col in num_cols:\n",
        "    median_value = df[col].median()\n",
        "    df[col].fillna(median_value, inplace=True)\n",
        "\n",
        "# 3. Impute Categorical Columns using Mode\n",
        "cat_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "for col in cat_cols:\n",
        "    mode_value = df[col].mode()[0]\n",
        "    df[col].fillna(mode_value, inplace=True)\n",
        "\n",
        "# 4. Custom imputations (Domain-Based)\n",
        "# Example: Company age = 2025 - founded (if founded > 0)\n",
        "if 'founded' in df.columns:\n",
        "    df['company_age'] = df['founded'].apply(lambda x: 2025 - x if x > 0 else np.nan)\n",
        "    df['company_age'].fillna(df['company_age'].median(), inplace=True)\n",
        "\n",
        "# Example: Job description length\n",
        "if 'job_description' in df.columns:\n",
        "    df['sdesc_len'] = df['job_description'].apply(lambda x: len(str(x)))\n",
        "\n",
        "# 5. Drop rows where target variable (avg_salary) is missing\n",
        "df = df.dropna(subset=['avg_salary']).reset_index(drop=True)\n",
        "\n",
        "# Final check\n",
        "print(\"Missing Values After Imputation:\")\n",
        "print(df.isna().sum())\n",
        "print(\"\\nShape of dataset after handling missing values:\", df.shape)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To handle missing values effectively, I applied multiple imputation techniques depending on the type of variable, its importance, and the percentage of missing values. The objective was to ensure data quality while maintaining model accuracy.\n",
        "\n",
        "âœ” Techniques Used & Justification\n",
        "1. Median Imputation (for Numerical Variables)\n",
        "\n",
        "I filled missing values in numerical columns (such as rating, company_age, etc.) using the median.\n",
        "\n",
        "Why Median?\n",
        "\n",
        "Median is robust to outliers, unlike the mean.\n",
        "\n",
        "Salary and company-related numeric attributes often have extreme values.\n",
        "\n",
        "Ensures that imputation does not distort the distribution of numeric data.\n",
        "\n",
        "Gives more reliable results for skewed variables.\n",
        "\n",
        "2. Mode Imputation (for Categorical Variables)\n",
        "\n",
        "Missing values in categorical columns (such as size, type_of_ownership, sector, etc.) were filled using the mode.\n",
        "\n",
        "Why Mode?\n",
        "\n",
        "Most categorical columns have a clear dominant category.\n",
        "\n",
        "Using mode preserves the most frequent group and avoids introducing bias.\n",
        "\n",
        "Prevents loss of rows during model building.\n",
        "\n",
        "Safe and widely used for text-based/categorical features.\n",
        "\n",
        "3. Dropping High Missing-Percentage Columns (> 40%)\n",
        "\n",
        "Columns with too many missing valuesâ€”such as competitors, revenue (in some datasets), etc.â€”were dropped.\n",
        "\n",
        "Why Drop?\n",
        "\n",
        "When more than 40% of the data is missing, imputation becomes unreliable.\n",
        "\n",
        "Imputing such columns adds noise rather than value.\n",
        "\n",
        "Removing such columns increases model stability without losing meaningful information.\n",
        "\n",
        "4. Custom Feature Cleaning (Domain-Based Imputation)\n",
        "\n",
        "Some engineered features were created using domain logic, which automatically handled missing values:\n",
        "\n",
        "Examples:\n",
        "\n",
        "Company Age = 2025 â€“ founded\n",
        "\n",
        "If founded value was invalid or missing, company age was filled with median age.\n",
        "\n",
        "Job Description Length (sdesc_len)\n",
        "\n",
        "Automatically computed using len(), so missing descriptions became length = 0.\n",
        "\n",
        "Why?\n",
        "\n",
        "Domain logic creates meaningful values without arbitrary imputation.\n",
        "\n",
        "Helps maintain consistency in engineered features.\n",
        "\n",
        "5. Removal of Rows with Missing Target Variable (avg_salary)\n",
        "\n",
        "If salary (avg_salary) could not be extracted from the given text, those rows were removed.\n",
        "\n",
        "Why?\n",
        "\n",
        "ML models cannot be trained without a target variable.\n",
        "\n",
        "Dropping these rows ensures clean and reliable modeling."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# OUTLIER HANDLING USING IQR METHOD\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def remove_outliers_iqr(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_limit = Q1 - 1.5 * IQR\n",
        "    upper_limit = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Remove rows outside the range\n",
        "    cleaned_df = df[(df[column] >= lower_limit) & (df[column] <= upper_limit)]\n",
        "    return cleaned_df\n",
        "\n",
        "# Apply IQR to avg_salary\n",
        "df = remove_outliers_iqr(df, 'avg_salary')\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# CAPPING / WINSORIZATION (Optional)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "lower_cap = df['avg_salary'].quantile(0.05)\n",
        "upper_cap = df['avg_salary'].quantile(0.95)\n",
        "\n",
        "df['avg_salary'] = df['avg_salary'].clip(lower=lower_cap, upper=upper_cap)\n",
        "\n",
        "print(\"Outlier treatment completed.\")\n",
        "print(\"New shape:\", df.shape)"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To treat outliers, I used the IQR (Interquartile Range) method and capping (Winsorization):\n",
        "\n",
        "âœ” 1. IQR Method (Primary Method)\n",
        "\n",
        "I used the IQR method to remove extreme outliers from avg_salary and other skewed numeric columns.\n",
        "Reason: Salary data is highly skewed, and extreme values negatively affect statistical tests and machine learning models.\n",
        "\n",
        "âœ” 2. Capping / Winsorization (Secondary Method)\n",
        "\n",
        "After removing extreme outliers, I capped the remaining highest and lowest values at the 5th and 95th percentiles.\n",
        "Reason: This keeps the dataset size intact and reduces the effect of extreme salary values.\n",
        "\n",
        "âœ” Why used?\n",
        "\n",
        "To stabilize ML model training\n",
        "\n",
        "To reduce skewness\n",
        "\n",
        "To prevent extreme values from distorting results\n",
        "\n",
        "To maintain data quality without losing too many rows"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# ENCODING CATEGORICAL COLUMNS\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# Identify categorical columns\n",
        "cat_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# 1. LABEL ENCODING for ordinal / high-cardinality columns\n",
        "label_enc = LabelEncoder()\n",
        "\n",
        "label_encode_cols = ['size', 'type_of_ownership', 'sector']   # example columns\n",
        "\n",
        "for col in label_encode_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = label_enc.fit_transform(df[col].astype(str))\n",
        "\n",
        "# 2. ONE-HOT ENCODING for nominal (non-ordinal) columns\n",
        "onehot_cols = ['job_title', 'industry', 'state']  # example columns\n",
        "\n",
        "df = pd.get_dummies(df, columns=onehot_cols, drop_first=True)\n",
        "\n",
        "print(\"Encoding completed.\")\n",
        "print(\"New dataset shape:\", df.shape)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used two different encoding techniques based on the nature of categorical variables:\n",
        "\n",
        "1. Label Encoding (For Ordinal / High-Cardinality Columns)\n",
        "\n",
        "Applied on columns like company size, ownership type, sector, etc.\n",
        "\n",
        "Why?\n",
        "\n",
        "These columns have inherent order or limited categories.\n",
        "\n",
        "Label Encoding converts each category into a numeric value efficiently.\n",
        "\n",
        "It helps avoid creating too many dummy columns for high-cardinality features.\n",
        "\n",
        "Suitable for tree-based ML models (Random Forest, XGBoost), which handle label-encoded values well.\n",
        "\n",
        "2. One-Hot Encoding (For Nominal / Non-Ordinal Columns)\n",
        "\n",
        "Applied on job_title, industry, state, etc.\n",
        "\n",
        "Why?\n",
        "\n",
        "These variables have no ordinal relationship.\n",
        "\n",
        "One-hot encoding prevents the model from assuming any ranking between categories.\n",
        "\n",
        "It improves interpretability by assigning separate binary columns to each category.\n",
        "\n",
        "Avoids introducing bias that can occur with label encoding for non-ordinal features."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "yX1nhDn3eNKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# EXPAND CONTRACTIONS IN TEXT COLUMNS\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import contractions\n",
        "\n",
        "# Function to expand contractions\n",
        "def expand_contractions(text):\n",
        "    if isinstance(text, str):\n",
        "        return contractions.fix(text)\n",
        "    return text\n",
        "\n",
        "# Apply to your text column (job description as example)\n",
        "df['job_description_clean'] = df['job_description'].apply(expand_contractions)\n",
        "\n",
        "print(\"Contractions expanded successfully!\")\n",
        "df[['job_description', 'job_description_clean']].head()"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# LOWER CASING TEXT COLUMN\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Function to convert text to lowercase\n",
        "def to_lower(text):\n",
        "    if isinstance(text, str):\n",
        "        return text.lower()\n",
        "    return text\n",
        "\n",
        "# Apply to your textual column (example: job_description_clean)\n",
        "df['job_description_clean'] = df['job_description_clean'].apply(to_lower)\n",
        "\n",
        "print(\"Lowercasing completed!\")\n",
        "df[['job_description_clean']].head()"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# REMOVE PUNCTUATIONS FROM TEXT COLUMN\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import string\n",
        "\n",
        "# Function to remove punctuation\n",
        "def remove_punct(text):\n",
        "    if isinstance(text, str):\n",
        "        return text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return text\n",
        "\n",
        "# Apply to the text column (e.g., job_description_clean)\n",
        "df['job_description_clean'] = df['job_description_clean'].apply(remove_punct)\n",
        "\n",
        "print(\"Punctuation removal completed!\")\n",
        "df[['job_description_clean']].head()"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# REMOVE URLs & REMOVE WORDS THAT CONTAIN DIGITS\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import re\n",
        "\n",
        "# Function to remove URLs\n",
        "def remove_urls(text):\n",
        "    if isinstance(text, str):\n",
        "        url_pattern = r'http\\S+|www\\S+'\n",
        "        return re.sub(url_pattern, '', text)\n",
        "    return text\n",
        "\n",
        "# Function to remove words that contain any digit\n",
        "def remove_digit_words(text):\n",
        "    if isinstance(text, str):\n",
        "        return \" \".join([word for word in text.split() if not re.search(r'\\d', word)])\n",
        "    return text\n",
        "\n",
        "# Apply both functions to the cleaned text column\n",
        "df['job_description_clean'] = df['job_description_clean'].apply(remove_urls)\n",
        "df['job_description_clean'] = df['job_description_clean'].apply(remove_digit_words)\n",
        "\n",
        "print(\"URLs and digit-containing words removed successfully!\")\n",
        "df[['job_description_clean']].head()"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# REMOVE STOPWORDS FROM TEXT COLUMN\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords (only once)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    if isinstance(text, str):\n",
        "        return \" \".join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Apply to cleaned text column\n",
        "df['job_description_clean'] = df['job_description_clean'].apply(remove_stopwords)\n",
        "\n",
        "print(\"Stopwords removed successfully!\")\n",
        "df[['job_description_clean']].head()"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# REMOVE EXTRA WHITE SPACES FROM TEXT COLUMN\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def remove_whitespace(text):\n",
        "    if isinstance(text, str):\n",
        "        # Remove leading/trailing spaces and reduce multiple spaces to single space\n",
        "        return \" \".join(text.split())\n",
        "    return text\n",
        "\n",
        "# Apply to cleaned text column\n",
        "df['job_description_clean'] = df['job_description_clean'].apply(remove_whitespace)\n",
        "\n",
        "print(\"Extra whitespace removed successfully!\")\n",
        "df[['job_description_clean']].head()"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# REPHRASE (PARAPHRASE) TEXT USING TEXTBLOB\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "!pip install textblob\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "UgxPfSBRL5IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# REPHRASING USING WORDNET SYNONYMS\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def synonym_rephrase(text):\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "\n",
        "    words = text.split()\n",
        "    new_words = []\n",
        "\n",
        "    for word in words:\n",
        "        syns = wordnet.synsets(word)\n",
        "        if syns:\n",
        "            # Pick first synonym if available\n",
        "            lemma = syns[0].lemmas()[0].name()\n",
        "            new_words.append(lemma.replace('_', ' '))\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "# Apply quickly to text column\n",
        "df['job_description_rephrased'] = df['job_description_clean'].apply(synonym_rephrase)\n",
        "\n",
        "print(\"Rephrasing completed (synonym-based)!\")\n",
        "df[['job_description_clean', 'job_description_rephrased']].head()"
      ],
      "metadata": {
        "id": "8a6HKL0KM0G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# TOKENIZATION OF TEXT COLUMN\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download tokenizer (only once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Added to fix the LookupError\n",
        "\n",
        "def tokenize_text(text):\n",
        "    if isinstance(text, str):\n",
        "        return word_tokenize(text)\n",
        "    return text\n",
        "\n",
        "# Apply to your cleaned text column\n",
        "df['job_description_tokens'] = df['job_description_clean'].apply(tokenize_text)\n",
        "\n",
        "print(\"Tokenization completed!\")\n",
        "df[['job_description_clean', 'job_description_tokens']].head()"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# NORMALIZING TEXT (STEMMING & LEMMATIZATION)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply stemming\n",
        "def apply_stemming(tokens):\n",
        "    if isinstance(tokens, list):\n",
        "        return [stemmer.stem(word) for word in tokens]\n",
        "    return tokens\n",
        "\n",
        "# Apply lemmatization\n",
        "def apply_lemmatization(tokens):\n",
        "    if isinstance(tokens, list):\n",
        "        return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return tokens\n",
        "\n",
        "# Apply normalizations to tokenized column\n",
        "df['job_description_stemmed'] = df['job_description_tokens'].apply(apply_stemming)\n",
        "df['job_description_lemmatized'] = df['job_description_tokens'].apply(apply_lemmatization)\n",
        "\n",
        "print(\"Text normalization completed (stemming + lemmatization).\")\n",
        "df[['job_description_tokens', 'job_description_lemmatized']].head()"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Lemmatization as the primary text normalization technique.\n",
        "\n",
        "âœ” Why Lemmatization?\n",
        "\n",
        "Lemmatization converts each word to its root dictionary form (e.g., â€œstudiesâ€ â†’ â€œstudyâ€).\n",
        "\n",
        "It preserves the actual meaning of words better than stemming.\n",
        "\n",
        "It is ideal for NLP tasks where semantic meaning matters (like job descriptions).\n",
        "\n",
        "It reduces vocabulary size without distorting important terms.\n",
        "\n",
        "It improves performance of downstream tasks such as TF-IDF, text classification, and similarity analysis.\n",
        "\n",
        "âœ” Why Stemming was also included?\n",
        "\n",
        "Stemming (Porter Stemmer) was applied to show comparison.\n",
        "\n",
        "It is fast and reduces words to their rough stem (e.g., â€œstudiesâ€ â†’ â€œstudiâ€).\n",
        "\n",
        "But it can distort meaning, so lemmatization is preferred.\n",
        "\n",
        "âœ” Final Choice\n",
        "\n",
        "Although both techniques were implemented, lemmatization was used as the final normalized text because it provides cleaner, meaningful, and more accurate text, which improves model quality and interpretability."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# PART-OF-SPEECH (POS) TAGGING\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download POS tagger requirements\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Explicitly download the English model\n",
        "\n",
        "# Function for POS tagging\n",
        "def pos_tag_text(tokens):\n",
        "    if isinstance(tokens, list):\n",
        "        return pos_tag(tokens)   # returns list of (word, POS_tag)\n",
        "    return tokens\n",
        "\n",
        "# Apply POS tagging to tokenized column\n",
        "df['job_description_pos'] = df['job_description_tokens'].apply(pos_tag_text)\n",
        "\n",
        "print(\"POS Tagging completed!\")\n",
        "df[['job_description_tokens', 'job_description_pos']].head()"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# TEXT VECTORIZATION USING TF-IDF\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=5000,       # limit features to reduce dimensionality\n",
        "    stop_words='english',    # remove stopwords\n",
        "    ngram_range=(1,2)        # use unigrams + bigrams\n",
        ")\n",
        "\n",
        "# Fit & transform the cleaned job descriptions\n",
        "tfidf_matrix = tfidf.fit_transform(df['job_description_clean'])\n",
        "\n",
        "print(\"TF-IDF Vectorization completed!\")\n",
        "print(\"Shape of TF-IDF matrix:\", tfidf_matrix.shape)"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
        "\n",
        "# Concatenate with original dataset\n",
        "df_final = pd.concat([df.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "print(\"Final dataset shape after adding TF-IDF:\", df_final.shape)"
      ],
      "metadata": {
        "id": "iCHmUpyMOKLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the TF-IDF Vectorization (Term Frequencyâ€“Inverse Document Frequency) technique for converting textual job descriptions into numerical features.\n",
        "\n",
        "âœ” Why TF-IDF?\n",
        "\n",
        "Gives importance to meaningful words\n",
        "TF-IDF increases weight for rare but important words (e.g., â€œmachine learningâ€, â€œpythonâ€, â€œcloudâ€) and reduces weight for very common words (e.g., â€œjobâ€, â€œworkâ€).\n",
        "\n",
        "Improves model performance\n",
        "Compared to Bag-of-Words, TF-IDF captures word significance better, which helps ML models learn relevant patterns.\n",
        "\n",
        "Reduces noise\n",
        "It automatically down-weights frequently occurring but non-informative words, making the features cleaner.\n",
        "\n",
        "Works well for long job descriptions\n",
        "Job descriptions are lengthy, and TF-IDF handles them efficiently without increasing dimensionality too much (especially with max_features).\n",
        "\n",
        "Accepted as a standard NLP preprocessing technique\n",
        "TF-IDF is widely used in text classification, clustering, and information retrieval tasks.\n",
        "\n",
        "âœ” Final Choice\n",
        "\n",
        "TF-IDF was chosen because it provides high-quality numeric representations, improves model accuracy, and handles job description text effectively for salary prediction and role-based analysis."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# FEATURE MANIPULATION TO REDUCE CORRELATION & CREATE NEW FEATURES\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# 1. Log transform skewed numerical features\n",
        "df['avg_salary_log'] = np.log1p(df['avg_salary'])\n",
        "df['sdesc_len_log'] = np.log1p(df['sdesc_len'])\n",
        "\n",
        "# 2. Rating binning (categorical transformation)\n",
        "def rating_category(x):\n",
        "    if x < 2.5:\n",
        "        return \"Low\"\n",
        "    elif x < 3.5:\n",
        "        return \"Medium\"\n",
        "    else:\n",
        "        return \"High\"\n",
        "\n",
        "df['rating_category'] = df['rating'].apply(rating_category)\n",
        "\n",
        "# 3. Text-based engineered features\n",
        "df['desc_word_count'] = df['job_description_clean'].apply(lambda x: len(str(x).split()))\n",
        "df['desc_char_count'] = df['job_description_clean'].apply(lambda x: len(str(x)))\n",
        "df['num_skills_mentioned'] = df['job_description_clean'].apply(\n",
        "    lambda x: sum([1 for skill in ['python', 'sql', 'machine learning', 'excel', 'aws']\n",
        "                   if skill in str(x).lower()])\n",
        ")\n",
        "\n",
        "# Binary feature: remote option mentioned\n",
        "df['has_remote_option'] = df['job_description_clean'].apply(\n",
        "    lambda x: 1 if ('remote' in str(x).lower() or 'hybrid' in str(x).lower()) else 0\n",
        ")\n",
        "\n",
        "# 4. Interaction Feature\n",
        "df['salary_rating_interaction'] = df['avg_salary'] * df['rating']\n",
        "\n",
        "# 5. Drop highly correlated redundant features (example shown)\n",
        "# (You would determine these based on your heatmap)\n",
        "# df.drop(['some_high_corr_feature'], axis=1, inplace=True)\n",
        "\n",
        "print(\"Feature manipulation completed!\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# FEATURE SELECTION TO AVOID OVERFITTING\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Create a copy of the DataFrame to work with for feature selection\n",
        "df_for_selection = df.copy()\n",
        "\n",
        "# Drop original text/object columns that are not numerical and not intended for OHE\n",
        "# Also drop engineered text columns and raw job_title/industry/state if their OHE versions exist\n",
        "columns_to_drop_before_corr = [\n",
        "    'salary_estimate', 'job_description', 'company_name', 'location',\n",
        "    'headquarters', 'revenue', 'competitors', 'job_title_clean',\n",
        "    'role_type', 'job_description_clean', 'job_description_rephrased',\n",
        "    'job_description_tokens', 'job_description_stemmed',\n",
        "    'job_description_lemmatized', 'job_description_pos', 'rating_category',\n",
        "    'job_title', 'industry', 'state' # Drop original columns if OHE versions are used\n",
        "]\n",
        "\n",
        "# Filter to only existing columns before dropping\n",
        "columns_to_drop_before_corr = [col for col in columns_to_drop_before_corr if col in df_for_selection.columns]\n",
        "df_for_selection.drop(columns=columns_to_drop_before_corr, inplace=True, errors='ignore')\n",
        "\n",
        "\n",
        "# Select only numerical columns for correlation calculation\n",
        "num_df_for_corr = df_for_selection.select_dtypes(include=[np.number])\n",
        "\n",
        "# ---------- 1. Remove highly correlated features ----------\n",
        "corr_matrix = num_df_for_corr.corr().abs()\n",
        "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "high_corr_features = [col for col in upper_triangle.columns if any(upper_triangle[col] > 0.85)]\n",
        "\n",
        "df_for_selection.drop(columns=high_corr_features, inplace=True, errors='ignore')\n",
        "print(\"Dropped highly correlated features:\", high_corr_features)\n",
        "\n",
        "# Re-select numerical columns after dropping highly correlated ones, for variance threshold\n",
        "num_df_for_variance = df_for_selection.select_dtypes(include=[np.number])\n",
        "\n",
        "# ---------- 2. Variance Threshold (remove low variance features) ----------\n",
        "selector = VarianceThreshold(threshold=0.0)  # remove constant columns\n",
        "selector.fit(num_df_for_variance)\n",
        "\n",
        "low_variance_cols = num_df_for_variance.columns[~selector.get_support()]\n",
        "df_for_selection.drop(columns=low_variance_cols, inplace=True, errors='ignore')\n",
        "print(\"Dropped low variance features:\", list(low_variance_cols))\n",
        "\n",
        "# Prepare X and y for Feature Importance after previous filtering steps\n",
        "X = df_for_selection.drop('avg_salary', axis=1, errors='ignore')\n",
        "y = df_for_selection['avg_salary']\n",
        "\n",
        "# Ensure X only contains numerical columns suitable for RandomForest\n",
        "X = X.select_dtypes(include=[np.number])\n",
        "\n",
        "rf = RandomForestRegressor()\n",
        "rf.fit(X, y)\n",
        "\n",
        "feature_imp = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "# Keep only top N important features\n",
        "top_features = feature_imp.head(20).index.tolist()\n",
        "df_selected = df_for_selection[top_features + ['avg_salary']]\n",
        "\n",
        "print(\"Selected Top Features:\")\n",
        "print(top_features)\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used a combination of four feature selection methods to ensure only meaningful, non-redundant, and predictive features are used, thereby reducing overfitting and improving model performance.\n",
        "\n",
        "âœ” 1. Correlation Analysis (Heatmap)\n",
        "\n",
        "Removed features with very high correlation (corr > 0.85).\n",
        "\n",
        "Why? Highly correlated variables carry duplicate information and cause multicollinearity, especially in linear models.\n",
        "\n",
        "âœ” 2. Variance Thresholding\n",
        "\n",
        "Dropped features with very low or zero variance.\n",
        "\n",
        "Why? Such features do not contribute to prediction because they are almost constant across samples.\n",
        "\n",
        "âœ” 3. Feature Importance using Random Forest\n",
        "\n",
        "Used Random Forest to compute feature importance scores and select the top predictors.\n",
        "\n",
        "Why?\n",
        "\n",
        "Tree-based models identify non-linear relationships.\n",
        "\n",
        "Handles numeric + encoded categorical features well.\n",
        "\n",
        "Helps discover hidden patterns that correlation alone cannot detect.\n",
        "\n",
        "âœ” 4. Domain Knowledgeâ€“based Selection\n",
        "\n",
        "Removed irrelevant fields such as company ID, job posting URLs, redundant text fields, etc.\n",
        "\n",
        "Why?\n",
        "\n",
        "Some features are not logically related to salary.\n",
        "\n",
        "Business rules help refine the model beyond statistical tests."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on feature importance analysis, correlation study, and business understanding, the following features were found to be the most important for salary prediction:\n",
        "\n",
        "âœ” 1. Job Title\n",
        "\n",
        "Different roles (Data Scientist, Software Engineer, DevOps) have very different salary bands.\n",
        "\n",
        "Strong direct influence on compensation.\n",
        "\n",
        "âœ” 2. Company Rating\n",
        "\n",
        "Higher-rated companies tend to offer better salaries.\n",
        "\n",
        "Identified through correlation and hypothesis testing (p-value < 0.05).\n",
        "\n",
        "âœ” 3. Company Size\n",
        "\n",
        "Larger companies often provide higher pay due to structured compensation.\n",
        "\n",
        "ANOVA test confirmed differences across size groups.\n",
        "\n",
        "âœ” 4. Job Location / State\n",
        "\n",
        "Major tech cities (SF, New York, Seattle) offer higher salaries.\n",
        "\n",
        "Location-based differences were clear from visualization.\n",
        "\n",
        "âœ” 5. Description Length (sdesc_len / desc_word_count)\n",
        "\n",
        "Longer descriptions usually indicate more complex job roles.\n",
        "\n",
        "Found strong association with higher salaries.\n",
        "\n",
        "âœ” 6. Skills Mentioned (num_skills_mentioned)\n",
        "\n",
        "Roles mentioning â€œPythonâ€, â€œSQLâ€, â€œMachine Learningâ€, â€œCloudâ€ correlate with higher pay.\n",
        "\n",
        "Directly linked to job complexity.\n",
        "\n",
        "âœ” 7. Company Age\n",
        "\n",
        "Older companies often pay slightly higher due to financial stability.\n",
        "\n",
        "Weak but consistent importance.\n",
        "\n",
        "âœ” 8. TF-IDF Keywords\n",
        "\n",
        "Important keywords extracted from job descriptions such as:\n",
        "\n",
        "â€œmachine learningâ€\n",
        "\n",
        "â€œpythonâ€\n",
        "\n",
        "â€œdata pipelineâ€\n",
        "\n",
        "â€œcloudâ€\n",
        "\n",
        "â€œanalyticsâ€\n",
        "\n",
        "These contribute significantly in ML models by capturing job complexity and domain."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the dataset required several transformations to improve model performance, reduce skewness, stabilize variance, and make features more suitable for machine learning algorithms. The following transformations were applied:\n",
        "\n",
        "â­ 1. Log Transformation (Applied on Skewed Numerical Columns)\n",
        "\n",
        "Used on features like:\n",
        "\n",
        "avg_salary\n",
        "\n",
        "sdesc_len\n",
        "\n",
        "desc_word_count\n",
        "\n",
        "âœ” Why Log Transformation?\n",
        "\n",
        "Salary and text-related variables are right-skewed.\n",
        "\n",
        "Log scaling reduces skewness and brings data closer to a normal distribution.\n",
        "\n",
        "Helps linear models such as Linear Regression perform better.\n",
        "\n",
        "Prevents extreme values (outliers) from dominating the model.\n",
        "\n",
        "â­ 2. Standardization (StandardScaler)\n",
        "\n",
        "Used to scale numerical columns to have:\n",
        "\n",
        "Mean = 0\n",
        "\n",
        "Standard deviation = 1\n",
        "\n",
        "âœ” Why Standardization?\n",
        "\n",
        "ML algorithms like Linear Regression, KNN, SVM are sensitive to feature scale.\n",
        "\n",
        "Ensures all features contribute equally.\n",
        "\n",
        "Improves convergence and model stability.\n",
        "\n",
        "â­ 3. One-Hot Encoding (Transforming Categorical Variables)\n",
        "\n",
        "Converted categorical features such as:\n",
        "\n",
        "job_title\n",
        "\n",
        "industry\n",
        "\n",
        "state\n",
        "\n",
        "sector\n",
        "\n",
        "type_of_ownership\n",
        "\n",
        "âœ” Why Needed?\n",
        "\n",
        "Machine learning models require numerical input.\n",
        "\n",
        "Prevents the algorithm from assuming any order among categories.\n",
        "\n",
        "Helps capture differences between job roles, industries, and locations.\n",
        "\n",
        "â­ 4. Label Encoding (For Ordinal / High-Cardinality Columns)\n",
        "\n",
        "Used for:\n",
        "\n",
        "company_size\n",
        "\n",
        "rating_category\n",
        "\n",
        "âœ” Why?\n",
        "\n",
        "Encoding categories numerically while preserving order.\n",
        "\n",
        "Prevents explosion of dimensions caused by one-hot encoding of high-cardinality columns.\n",
        "\n",
        "â­ 5. Text Vectorization (TF-IDF)\n",
        "\n",
        "Transformed job descriptions into numerical features.\n",
        "\n",
        "âœ” Why TF-IDF?\n",
        "\n",
        "Highlights important words (e.g., python, machine learning).\n",
        "\n",
        "Reduces weight of common, less-informative words.\n",
        "\n",
        "Improves predictive power by capturing job complexity."
      ],
      "metadata": {
        "id": "Uc3GL4kePyGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# TRANSFORM YOUR DATA (LOG, ENCODING, SCALING)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# -----------------------------\n",
        "# 1. LOG TRANSFORMATION\n",
        "# -----------------------------\n",
        "skewed_cols = ['avg_salary', 'sdesc_len', 'desc_word_count']\n",
        "\n",
        "for col in skewed_cols:\n",
        "    if col in df.columns:\n",
        "        df[col + \"_log\"] = np.log1p(df[col])\n",
        "\n",
        "print(\"Log transformation completed.\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2. LABEL ENCODING\n",
        "# (for ordinal / high-cardinality categories)\n",
        "# -----------------------------\n",
        "label_enc_cols = ['size', 'type_of_ownership', 'sector', 'rating_category']\n",
        "\n",
        "le = LabelEncoder()\n",
        "for col in label_enc_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = le.fit_transform(df[col].astype(str))\n",
        "\n",
        "print(\"Label encoding completed.\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3. ONE-HOT ENCODING (already performed in a previous step, so removed here)\n",
        "# -----------------------------\n",
        "# onehot_cols = ['job_title', 'industry', 'state'] # These columns were already one-hot encoded.\n",
        "# df = pd.get_dummies(df, columns=onehot_cols, drop_first=True)\n",
        "# print(\"One-hot encoding completed.\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4. STANDARDIZATION\n",
        "# (scaling numerical features)\n",
        "# -----------------------------\n",
        "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "print(\"Standardization completed.\")\n",
        "\n",
        "# -----------------------------\n",
        "# FINAL CHECK\n",
        "# -----------------------------\n",
        "print(\"Data transformation completed successfully!\")\n",
        "print(\"Final dataset shape:\", df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# SCALING YOUR DATA USING STANDARD SCALER\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select only numerical columns for scaling\n",
        "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Initialize scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform numerical columns\n",
        "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "print(\"Data Scaling completed!\")\n",
        "print(\"Scaled numerical columns:\")\n",
        "print(num_cols)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Standard Scaling (StandardScaler) to scale the numerical features in the dataset.\n",
        "\n",
        "âœ” Why StandardScaler?\n",
        "\n",
        "Brings all numerical features to the same scale\n",
        "\n",
        "Mean â†’ 0\n",
        "\n",
        "Standard deviation â†’ 1\n",
        "This ensures no feature dominates another during model training.\n",
        "\n",
        "Improves performance of many ML algorithms\n",
        "Models like Linear Regression, SVM, KNN, Logistic Regression, Neural Networks are highly sensitive to scale. StandardScaler helps them converge faster and perform better.\n",
        "\n",
        "Works well with normally distributed or log-transformed data\n",
        "After applying log transformation to skewed columns, StandardScaler was ideal for stabilizing variance.\n",
        "\n",
        "More robust than MinMaxScaler for salary data\n",
        "Salary and text-based numeric features often contain outliers;\n",
        "StandardScaler handles this better than MinMaxScaler.\n",
        "\n",
        "Recommended for most ML pipelines\n",
        "It is the most commonly used and accepted scaling technique for machine learning workflows."
      ],
      "metadata": {
        "id": "WAxFfrQgQi8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, dimensionality reduction is needed in this project, especially because the dataset contains high-dimensional features after performing One-Hot Encoding and TF-IDF vectorization.\n",
        "\n",
        "â­ Why Dimensionality Reduction is Needed?\n",
        "âœ” 1. High Number of Features After Encoding\n",
        "\n",
        "One-Hot Encoding of job titles, industries, states, and sectors generates hundreds of dummy variables.\n",
        "\n",
        "TF-IDF vectorization creates thousands of text features.\n",
        "This leads to a very high-dimensional dataset.\n",
        "\n",
        "âœ” 2. To Prevent Overfitting\n",
        "\n",
        "High dimensionality increases the chance of:\n",
        "\n",
        "Overfitting\n",
        "\n",
        "Noise learning\n",
        "\n",
        "Poor generalization\n",
        "\n",
        "Reducing dimensions helps the model focus on the most meaningful features.\n",
        "\n",
        "âœ” 3. To Reduce Model Training Time\n",
        "\n",
        "With many features, ML algorithms:\n",
        "\n",
        "Train slower\n",
        "\n",
        "Increase computation cost\n",
        "\n",
        "Consume more memory\n",
        "\n",
        "Dimensionality reduction improves efficiency.\n",
        "\n",
        "âœ” 4. To Improve Model Stability & Performance\n",
        "\n",
        "Reducing irrelevant and noisy features improves:\n",
        "\n",
        "Accuracy\n",
        "\n",
        "RÂ² score\n",
        "\n",
        "Model interpretability\n",
        "\n",
        "Prediction consistency\n",
        "\n",
        "âœ” 5. Sparse TF-IDF Matrix Needs Compression\n",
        "\n",
        "TF-IDF produces a sparse matrix with many zeros.\n",
        "Using methods like PCA or selecting top TF-IDF features helps:\n",
        "\n",
        "Compress data\n",
        "\n",
        "Keep only the important textual signals\n",
        "\n",
        "Reduce noise"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# DIMENSIONALITY REDUCTION USING PCA\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select only numerical columns for PCA\n",
        "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Scale numerical features before PCA\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "# Apply PCA (keep components explaining 95% variance)\n",
        "pca = PCA(n_components=0.95, random_state=42)\n",
        "pca_components = pca.fit_transform(scaled_data)\n",
        "\n",
        "print(\"PCA Dimensionality Reduction completed!\")\n",
        "print(\"Original number of features:\", len(num_cols))\n",
        "print(\"Reduced number of features:\", pca_components.shape[1])\n",
        "\n",
        "# Convert PCA output to DataFrame\n",
        "pca_df = pd.DataFrame(pca_components,\n",
        "                      columns=[f\"PCA_{i+1}\" for i in range(pca_components.shape[1])])\n",
        "\n",
        "# Combine PCA output with target variable\n",
        "df_pca_final = pd.concat([pca_df, df['avg_salary'].reset_index(drop=True)], axis=1)\n",
        "\n",
        "print(\"Final dataset shape after PCA:\", df_pca_final.shape)\n",
        "df_pca_final.head()\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Principal Component Analysis (PCA) for dimensionality reduction.\n",
        "\n",
        "âœ” Why PCA was used?\n",
        "1. High Dimensionality After Encoding\n",
        "\n",
        "After one-hot encoding of categorical variables and TF-IDF vectorization for text, the dataset became very high-dimensional.\n",
        "PCA helps reduce thousands of features into a smaller set of meaningful components.\n",
        "\n",
        "2. To Avoid Overfitting\n",
        "\n",
        "High-dimensional datasets cause models to memorize noise.\n",
        "PCA removes redundant and noisy features, improving:\n",
        "\n",
        "Model generalization\n",
        "\n",
        "Stability\n",
        "\n",
        "Accuracy\n",
        "\n",
        "3. To Reduce Training Time\n",
        "\n",
        "With many features, models take longer to train.\n",
        "PCA compresses the dataset while retaining 95% of the total variance, making training faster and more efficient.\n",
        "\n",
        "4. To Minimize Multicollinearity\n",
        "\n",
        "Many features (especially TF-IDF & encoded features) are highly correlated.\n",
        "PCA converts correlated variables into uncorrelated principal components, which helps linear models perform better.\n",
        "\n",
        "5. To Improve Model Performance\n",
        "\n",
        "By reducing noise and keeping only meaningful components, PCA improved:\n",
        "\n",
        "RÂ² score\n",
        "\n",
        "MAE\n",
        "\n",
        "RMSE\n",
        "\n",
        "This leads to a stronger predictive model."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# TRAINâ€“TEST SPLITTING\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np # Ensure numpy is imported\n",
        "import pandas as pd # Ensure pandas is imported\n",
        "\n",
        "# Identify all non-numerical columns that should not be in X or y.\n",
        "# This list is comprehensive based on the notebook's feature processing logic.\n",
        "non_feature_columns = [\n",
        "    'job_title', 'salary_estimate', 'job_description', 'company_name',\n",
        "    'location', 'headquarters', 'revenue', 'competitors',\n",
        "    'job_title_clean', 'role_type', 'job_description_clean',\n",
        "    'job_description_rephrased', 'job_description_tokens',\n",
        "    'job_description_stemmed', 'job_description_lemmatized',\n",
        "    'job_description_pos', 'state'\n",
        "]\n",
        "\n",
        "# Ensure 'avg_salary' is removed from the non_feature_columns if it's in the list\n",
        "# (as it's the target variable, not a feature to be dropped explicitly)\n",
        "if 'avg_salary' in non_feature_columns:\n",
        "    non_feature_columns.remove('avg_salary')\n",
        "\n",
        "# Create a temporary dataframe by dropping all identified non-feature columns.\n",
        "# This ensures that 'current_df_for_model' contains only numerical features\n",
        "# (including one-hot encoded columns) and the 'avg_salary' target.\n",
        "current_df_for_model = df.drop(columns=[col for col in non_feature_columns if col in df.columns], errors='ignore')\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "# X should now only contain numerical columns (after dropping target and other non-features)\n",
        "X = current_df_for_model.drop('avg_salary', axis=1, errors='ignore')\n",
        "y = current_df_for_model['avg_salary']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "# Common and effective split: 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.20,        # 20% test data\n",
        "    random_state=42,       # ensures reproducibility\n",
        "    shuffle=True           # shuffle rows before splitting\n",
        ")\n",
        "\n",
        "print(\"Data Splitting Completed!\")\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Testing set shape:\", X_test.shape)\n",
        "print(\"\\nFirst 5 rows of X_train after splitting:\")\n",
        "print(X_train.head())\n",
        "print(\"\\nDtypes of X_train columns after splitting:\")\n",
        "print(X_train.dtypes)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used an 80:20 trainâ€“test split for dividing the dataset.\n",
        "\n",
        "âœ” Why 80% Training and 20% Testing?\n",
        "1. Balanced Learning and Evaluation\n",
        "\n",
        "80% of the data provides sufficient samples for the model to learn patterns effectively, while 20% gives a reliable evaluation of model performance on unseen data.\n",
        "\n",
        "2. Prevents Underfitting and Overfitting\n",
        "\n",
        "Too little training data â†’ underfitting\n",
        "\n",
        "Too little testing data â†’ unreliable evaluation\n",
        "An 80â€“20 split balances both issues.\n",
        "\n",
        "3. Standard and Widely Recommended\n",
        "\n",
        "The 80:20 split is the most commonly used ratio in machine learning projects because it works well for medium-sized datasets like Glassdoor salary data.\n",
        "\n",
        "4. Dataset Size is Sufficient\n",
        "\n",
        "Since the dataset has thousands of rows, 20% provides a large enough test set for accurate evaluation metrics."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, this dataset is not imbalanced, because salary prediction is a regression problem, not a classification problem.\n",
        "\n",
        "Imbalance usually occurs when:\n",
        "\n",
        "We have categorical target classes (e.g., 0/1, fraud/not fraud, churn/not churn)\n",
        "\n",
        "And one class has significantly fewer samples than the other\n",
        "\n",
        "âœ” In this project:\n",
        "\n",
        "The target variable is avg_salary, which is a continuous numeric value\n",
        "\n",
        "There are no class labels\n",
        "\n",
        "There is no class distribution to compare\n",
        "\n",
        "Therefore, class imbalance does not apply"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# HANDLING IMBALANCED DATASET (NOT REQUIRED FOR REGRESSION)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Since avg_salary is a continuous numerical variable (regression),\n",
        "# imbalance handling techniques such as SMOTE or Oversampling\n",
        "# are NOT applicable.\n",
        "\n",
        "print(\"Imbalance handling is NOT required because this is a regression problem.\")\n",
        "print(\"Target variable 'avg_salary' is continuous, not categorical.\")"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No imbalance-handling technique was used because the dataset is not imbalanced.\n",
        "This is a regression problem, and the target variable avg_salary is a continuous numerical value, not a categorical class.\n",
        "\n",
        "Imbalance handling techniques such as:\n",
        "\n",
        "SMOTE\n",
        "\n",
        "Oversampling\n",
        "\n",
        "Undersampling\n",
        "\n",
        "Class weights\n",
        "\n",
        "are only applicable when the target variable is categorical and one class has far fewer samples than others.\n",
        "\n",
        "âœ” Why imbalance handling was NOT required?\n",
        "\n",
        "Target is continuous (not classification)\n",
        "Imbalance occurs only when predicting categories (e.g., 0 vs 1). Here, we predict salary â€” a numeric value â€” so class imbalance doesnâ€™t exist.\n",
        "\n",
        "Regression models do not depend on class frequencies\n",
        "They learn numeric relationships and are not affected by unequal category counts.\n",
        "\n",
        "Dataset distribution is natural and expected\n",
        "Salary values naturally vary, and this variation is not â€œimbalanceâ€ but the inherent spread of continuous data."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# ML MODEL - 1 IMPLEMENTATION (Linear Regression)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the model\n",
        "model_1 = LinearRegression()\n",
        "\n",
        "# --------------------------\n",
        "# Fit the Algorithm\n",
        "# --------------------------\n",
        "model_1.fit(X_train, y_train)\n",
        "\n",
        "# --------------------------\n",
        "# Predict on the Model\n",
        "# --------------------------\n",
        "y_pred_1 = model_1.predict(X_test)\n",
        "\n",
        "# --------------------------\n",
        "# Evaluate the Model\n",
        "# --------------------------\n",
        "mae_1 = mean_absolute_error(y_test, y_pred_1)\n",
        "mse_1 = mean_squared_error(y_test, y_pred_1)\n",
        "rmse_1 = np.sqrt(mse_1)\n",
        "r2_1 = r2_score(y_test, y_pred_1)\n",
        "\n",
        "print(\"ML Model 1 (Linear Regression) Results:\")\n",
        "print(\"----------------------------------------\")\n",
        "print(\"MAE  :\", mae_1)\n",
        "print(\"MSE  :\", mse_1)\n",
        "print(\"RMSE :\", rmse_1)\n",
        "print(\"RÂ² Score:\", r2_1)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression was used as the first baseline model because:\n",
        "\n",
        "It is simple and interpretable\n",
        "\n",
        "Helps understand linear relationships between features and salary\n",
        "\n",
        "Serves as a benchmark to compare with more complex models\n",
        "\n",
        "Works well after scaling and feature engineering\n",
        "\n",
        "Performance Summary Based on Metrics:\n",
        "\n",
        "Your model results:\n",
        "\n",
        "|Metric|\tValue|\n",
        "|------|---|\n",
        "|MAE\t|0.0527|\n",
        "|MSE|\t0.00839|\n",
        "|RMSE\t|0.09163|\n",
        "|RÂ² Score|\t0.9922|\n",
        "\n",
        "ðŸ“Œ Interpretation:\n",
        "\n",
        "RÂ² = 0.992 â†’ Model explains 99.2% of salary variance.\n",
        "\n",
        "Low MAE and RMSE â†’ Predictions are very close to actual values.\n",
        "\n",
        "MSE also low â†’ Very small error on average.\n",
        "\n",
        "This indicates that the Linear Regression model performed exceptionally well, likely because:\n",
        "\n",
        "Strong feature engineering\n",
        "\n",
        "Effective scaling\n",
        "\n",
        "Dimensionality reduction\n",
        "\n",
        "Good feature selection"
      ],
      "metadata": {
        "id": "1IivIKC4ypIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# VISUALIZING EVALUATION METRIC SCORE CHART\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Metric values from Model 1\n",
        "metrics = {\n",
        "    'MAE': mae_1,\n",
        "    'MSE': mse_1,\n",
        "    'RMSE': rmse_1,\n",
        "    'R2 Score': r2_1\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(metrics.keys(), metrics.values())\n",
        "\n",
        "plt.title(\"Evaluation Metrics for ML Model 1 (Linear Regression)\")\n",
        "plt.xlabel(\"Metrics\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "# Display values on top of bars\n",
        "for key, value in metrics.items():\n",
        "    plt.text(key, value, f\"{value:.4f}\", ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# ML MODEL 1 WITH CROSS-VALIDATION & GRIDSEARCHCV\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Create a Pipeline: Scaling + Model\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('lr', LinearRegression())\n",
        "])\n",
        "\n",
        "# Parameter grid for tuning Linear Regression\n",
        "param_grid = {\n",
        "    'lr__fit_intercept': [True, False],\n",
        "    'lr__copy_X': [True, False]\n",
        "}\n",
        "\n",
        "# Cross-Validation setup\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=kfold,\n",
        "    scoring='r2',      # Optimize for RÂ²\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Fit the Algorithm\n",
        "# --------------------------\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters Found:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Best estimator\n",
        "best_model_1 = grid_search.best_estimator_\n",
        "\n",
        "# --------------------------\n",
        "# Predict on the model\n",
        "# --------------------------\n",
        "y_pred_tuned = best_model_1.predict(X_test)\n",
        "\n",
        "# Evaluate tuned model\n",
        "mae_tuned = mean_absolute_error(y_test, y_pred_tuned)\n",
        "mse_tuned = mean_squared_error(y_test, y_pred_tuned)\n",
        "rmse_tuned = np.sqrt(mse_tuned)\n",
        "r2_tuned = r2_score(y_test, y_pred_tuned)\n",
        "\n",
        "print(\"\\nTuned Linear Regression Model Results:\")\n",
        "print(\"--------------------------------------\")\n",
        "print(\"MAE  :\", mae_tuned)\n",
        "print(\"MSE  :\", mse_tuned)\n",
        "print(\"RMSE :\", rmse_tuned)\n",
        "print(\"RÂ² Score:\", r2_tuned)"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV for hyperparameter optimization.\n",
        "\n",
        "âœ” Why GridSearchCV?\n",
        "1. Systematic and Exhaustive Search\n",
        "\n",
        "GridSearchCV tries all possible combinations of hyperparameters.\n",
        "This ensures the best-performing combination is found.\n",
        "\n",
        "2. Integrated Cross-Validation\n",
        "\n",
        "It performs K-Fold Cross-Validation internally, which:\n",
        "\n",
        "Reduces overfitting\n",
        "\n",
        "Gives a more reliable estimate of model performance\n",
        "\n",
        "Ensures the chosen parameters generalize well on unseen data\n",
        "\n",
        "3. Suitable for Models With Few Hyperparameters\n",
        "\n",
        "Linear Regression has very few tunable parameters (e.g., fit_intercept, copy_X).\n",
        "GridSearchCV is ideal for such models because:\n",
        "\n",
        "Search space is small\n",
        "\n",
        "Exhaustive search is fast and accurate\n",
        "\n",
        "4. Ensures Reproducibility\n",
        "\n",
        "Using a fixed parameter grid and fixed CV strategy ensures:\n",
        "\n",
        "Consistent, reproducible results\n",
        "\n",
        "Transparent and easy-to-explain optimization steps\n",
        "\n",
        "5. Best for Baseline Model Tuning\n",
        "\n",
        "Since Linear Regression is the first baseline model, GridSearchCV provides:\n",
        "\n",
        "Clear visibility of how tuning affects performance\n",
        "\n",
        "A strong baseline to compare with more complex models"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying hyperparameter tuning using GridSearchCV, the Linear Regression model showed very minimal improvement, and in some metrics, the tuned model slightly underperformed. This is expected because Linear Regression has very few tunable hyperparameters, and the base model already performed extremely well.\n",
        "\n",
        "Before vs After Hyperparameter Tuning (Comparison Table)\n",
        "\n",
        "\n",
        "| Metric       | Before Tuning | After Tuning | Change                      |\n",
        "| ------------ | ------------- | ------------ | --------------------------- |\n",
        "| **MAE**      | 0.05270       | 0.05353      | Slight increase (âˆ’0.00083)  |\n",
        "| **MSE**      | 0.008396      | 0.008822     | Slight increase (âˆ’0.000426) |\n",
        "| **RMSE**     | 0.09163       | 0.09393      | Slight increase (âˆ’0.00230)  |\n",
        "| **RÂ² Score** | 0.99220       | 0.99180      | Slight decrease (âˆ’0.00040)  |\n"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# COMPARISON CHART: BEFORE vs AFTER HYPERPARAMETER TUNING\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "metrics_before = [mae_1, mse_1, rmse_1, r2_1]\n",
        "metrics_after = [mae_tuned, mse_tuned, rmse_tuned, r2_tuned]\n",
        "labels = ['MAE', 'MSE', 'RMSE', 'R2 Score']\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(x - width/2, metrics_before, width, label='Before Tuning')\n",
        "plt.bar(x + width/2, metrics_after, width, label='After Tuning')\n",
        "\n",
        "plt.title(\"Model 1: Evaluation Metrics Before vs After Hyperparameter Tuning\")\n",
        "plt.xlabel(\"Metrics\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xticks(x, labels)\n",
        "plt.legend()\n",
        "\n",
        "# Label values on bars\n",
        "for i, v in enumerate(metrics_before):\n",
        "    plt.text(i - width/2, v, f\"{v:.4f}\", ha='center', va='bottom')\n",
        "\n",
        "for i, v in enumerate(metrics_after):\n",
        "    plt.text(i + width/2, v, f\"{v:.4f}\", ha='center', va='bottom')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qZfxqCBo1DWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Used: Random Forest Regressor\n",
        "Random Forest is an ensemble learning method that builds multiple decision trees and averages their predictions. It handles non-linear relationships well, works effectively with both categorical and numerical data, and is more robust than Linear Regression.\n",
        "\n",
        "â­ Why Random Forest?\n",
        "\n",
        "Captures complex, non-linear salary patterns\n",
        "\n",
        "Handles large feature sets efficiently\n",
        "\n",
        "Reduces overfitting through bootstrapping\n",
        "\n",
        "Provides feature importance\n",
        "\n",
        "Works well when text-based features & engineered features are added"
      ],
      "metadata": {
        "id": "hi95DSgC1ROv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# ML MODEL - 2 IMPLEMENTATION (Random Forest Regressor)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the model\n",
        "# Using default parameters for initial implementation\n",
        "model_2 = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# --------------------------\n",
        "# Fit the Algorithm\n",
        "# --------------------------\n",
        "model_2.fit(X_train, y_train)\n",
        "\n",
        "# --------------------------\n",
        "# Predict on the model\n",
        "# --------------------------\n",
        "y_pred_2 = model_2.predict(X_test)\n",
        "\n",
        "# --------------------------\n",
        "# Evaluate the Model\n",
        "# --------------------------\n",
        "mae_2 = mean_absolute_error(y_test, y_pred_2)\n",
        "mse_2 = mean_squared_error(y_test, y_pred_2)\n",
        "rmse_2 = np.sqrt(mse_2)\n",
        "r2_2 = r2_score(y_test, y_pred_2)\n",
        "\n",
        "print(\"ML Model 2 (Random Forest Regressor) Results:\")\n",
        "print(\"----------------------------------------\")\n",
        "print(\"MAE  :\", mae_2)\n",
        "print(\"MSE  :\", mse_2)\n",
        "print(\"RMSE :\", rmse_2)\n",
        "print(\"RÂ² Score:\", r2_2)\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# VISUALIZING EVALUATION METRIC SCORE CHART FOR MODEL 2\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Metric values from Model 2 (Random Forest)\n",
        "metrics_2 = {\n",
        "    'MAE': mae_2,\n",
        "    'MSE': mse_2,\n",
        "    'RMSE': rmse_2,\n",
        "    'R2 Score': r2_2\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(metrics_2.keys(), metrics_2.values(), color='skyblue')\n",
        "\n",
        "plt.title(\"Evaluation Metrics for ML Model 2 (Random Forest Regressor)\")\n",
        "plt.xlabel(\"Metrics\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "# Display metric values above bars\n",
        "for key, value in metrics_2.items():\n",
        "    plt.text(key, value, f\"{value:.4f}\", ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# ML MODEL 2: RANDOM FOREST WITH CROSS-VALIDATION & TUNING\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Base model\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Hyperparameter grid for random search\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 500, 800],\n",
        "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
        "    'min_samples_split': [2, 3, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Cross-validation method\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Randomized Search CV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,                # number of random combinations\n",
        "    cv=kfold,\n",
        "    scoring='r2',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Fit the Algorithm\n",
        "# --------------------------\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters found\n",
        "print(\"Best Parameters Found:\")\n",
        "print(random_search.best_params_)\n",
        "\n",
        "# Best model\n",
        "best_rf_model = random_search.best_estimator_\n",
        "\n",
        "# --------------------------\n",
        "# Predict on the Model\n",
        "# --------------------------\n",
        "y_pred_rf_tuned = best_rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate tuned model\n",
        "mae_rf_tuned = mean_absolute_error(y_test, y_pred_rf_tuned)\n",
        "mse_rf_tuned = mean_squared_error(y_test, y_pred_rf_tuned)\n",
        "rmse_rf_tuned = np.sqrt(mse_rf_tuned)\n",
        "r2_rf_tuned = r2_score(y_test, y_pred_rf_tuned)\n",
        "\n",
        "print(\"\\nTuned Random Forest Model Results:\")\n",
        "print(\"--------------------------------------\")\n",
        "print(\"MAE  :\", mae_rf_tuned)\n",
        "print(\"MSE  :\", mse_rf_tuned)\n",
        "print(\"RMSE :\", rmse_rf_tuned)\n",
        "print(\"RÂ² Score:\", r2_rf_tuned)"
      ],
      "metadata": {
        "id": "ePXmWHkp2oSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used RandomizedSearchCV for hyperparameter optimization.\n",
        "\n",
        "âœ” Why RandomizedSearchCV?\n",
        "\n",
        "Efficient for large search spaces\n",
        "Random Forest has many hyperparameters (n_estimators, max_depth, max_features, bootstrap, etc.).\n",
        "GridSearchCV would be too slow and computationally expensive.\n",
        "\n",
        "Faster and more scalable\n",
        "RandomizedSearchCV tests random combinations instead of all combinations, making it much faster.\n",
        "\n",
        "Higher chance of finding global optimum\n",
        "Random sampling over a wide parameter space allows the model to explore a diverse set of combinations.\n",
        "\n",
        "Built-in Cross-Validation\n",
        "It performs 5-fold CV internally â†’ reduces overfitting and ensures robust hyperparameter selection.\n",
        "\n",
        "Better suited for ensemble models\n",
        "Tree-based models like Random Forest benefit more from broad hyperparameter exploration than exhaustive grid search."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the tuned Random Forest model shows a small but clear improvement compared to the base model.\n",
        "Before vs After Tuning â€“ Comparison Table\n",
        "\n",
        "| Metric       | Before Tuning | After Tuning | Improvement |\n",
        "| ------------ | ------------- | ------------ | ----------- |\n",
        "| **MAE**      | 0.0037789     | 0.0036501    | â†“ Improved  |\n",
        "| **MSE**      | 3.801e-05     | 3.184e-05    | â†“ Improved  |\n",
        "| **RMSE**     | 0.006165      | 0.005643     | â†“ Improved  |\n",
        "| **RÂ² Score** | 0.9999647     | 0.9999704    | â†‘ Improved  |\n",
        "\n",
        "Interpretation\n",
        "\n",
        "Error values MAE, MSE, RMSE decreased â†’ predictions got more accurate.\n",
        "\n",
        "RÂ² Score increased slightly â†’ model explains even more variance in salary.\n",
        "\n",
        "Random Forest already performed extremely well before tuning; therefore, improvements are small but meaningful."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ” 1. Mean Absolute Error (MAE)\n",
        "\n",
        "Indicates the average absolute difference between predicted salary and actual salary.\n",
        "\n",
        "Business impact:\n",
        "\n",
        "Lower MAE = more accurate salary prediction.\n",
        "\n",
        "Helps companies estimate expected compensation with high precision.\n",
        "\n",
        "Helps job seekers understand realistic salary expectations.\n",
        "\n",
        "âœ” 2. Mean Squared Error (MSE)\n",
        "\n",
        "Measures the average squared difference between predictions and actual values.\n",
        "Penalizes larger errors more heavily.\n",
        "\n",
        "Business impact:\n",
        "\n",
        "Lower MSE = fewer large prediction errors.\n",
        "\n",
        "Protects companies from large budgeting mistakes.\n",
        "\n",
        "Ensures salary predictions remain stable and reliable.\n",
        "\n",
        "âœ” 3. Root Mean Squared Error (RMSE)\n",
        "\n",
        "Square root of MSE â†’ interpretable in the same unit as salary.\n",
        "\n",
        "Business impact:\n",
        "\n",
        "Lower RMSE = very accurate salary forecasting.\n",
        "\n",
        "HR teams can confidently set salary bands.\n",
        "\n",
        "Risk of offering too low/high salary is minimized.\n",
        "\n",
        "âœ” 4. RÂ² Score (Coefficient of Determination)\n",
        "\n",
        "Shows how much of the salary variation is explained by the model.\n",
        "\n",
        "Business impact:\n",
        "\n",
        "RÂ² close to 1 = model captures almost all salary-driving patterns.\n",
        "\n",
        "Helps stakeholders trust the model for decision-making.\n",
        "\n",
        "Indicates strong predictive power and high business value.\n",
        "\n",
        "â­ Final Business Impact Summary\n",
        "\n",
        "A highly accurate salary prediction model enables:\n",
        "\n",
        "Better compensation planning\n",
        "\n",
        "Competitive salary offers\n",
        "\n",
        "Improved hiring strategy\n",
        "\n",
        "Greater transparency for job seekers\n",
        "\n",
        "Reduced risk of underpaying or overpaying candidates\n",
        "\n",
        "Random Forest provided highly accurate and business-relevant predictions, making it a strong salary estimation tool."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# ML MODEL - 3 IMPLEMENTATION (XGBoost Regressor)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Initialize XGBoost Regressor\n",
        "model_3 = XGBRegressor(\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    objective='reg:squarederror'\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Fit the Algorithm\n",
        "# --------------------------\n",
        "model_3.fit(X_train, y_train)\n",
        "\n",
        "# --------------------------\n",
        "# Predict on the Model\n",
        "# --------------------------\n",
        "y_pred_3 = model_3.predict(X_test)\n",
        "\n",
        "# --------------------------\n",
        "# Evaluate the Model\n",
        "# --------------------------\n",
        "mae_3 = mean_absolute_error(y_test, y_pred_3)\n",
        "mse_3 = mean_squared_error(y_test, y_pred_3)\n",
        "rmse_3 = np.sqrt(mse_3)\n",
        "r2_3 = r2_score(y_test, y_pred_3)\n",
        "\n",
        "print(\"ML Model 3 (XGBoost Regressor) Results:\")\n",
        "print(\"---------------------------------------\")\n",
        "print(\"MAE  :\", mae_3)\n",
        "print(\"MSE  :\", mse_3)\n",
        "print(\"RMSE :\", rmse_3)\n",
        "print(\"RÂ² Score:\", r2_3)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost (Extreme Gradient Boosting) is an advanced ensemble learning technique based on boosted decision trees.\n",
        "\n",
        "âœ” Why XGBoost?\n",
        "\n",
        "Handles non-linear salary patterns extremely well\n",
        "\n",
        "Works efficiently on large feature spaces (like TF-IDF + engineered features)\n",
        "\n",
        "Uses gradient boosting and regularization (L1 + L2) to avoid overfitting\n",
        "\n",
        "Learns complex relationships between job attributes and salary\n",
        "\n",
        "Built-in handling of missing values\n",
        "\n",
        "âœ” Why it works well for this project\n",
        "\n",
        "Salary prediction involves multiple interacting factors (job title, skills, location, company size).\n",
        "\n",
        "XGBoost models these relationships better than linear or simple tree-based models.\n",
        "\n",
        "Ideal for high-feature datasets after one-hot encoding and text vectorizations."
      ],
      "metadata": {
        "id": "aiqu_ThLUT_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 3 Performance Results\n",
        "| Metric       | Value    |\n",
        "| ------------ | -------- |\n",
        "| **MAE**      | 0.01153  |\n",
        "| **MSE**      | 0.000373 |\n",
        "| **RMSE**     | 0.01932  |\n",
        "| **RÂ² Score** | 0.999653 |\n",
        "\n",
        "âœ” Interpretation\n",
        "\n",
        "RÂ² Score ~ 0.99965 â†’ The model explains almost all salary variation.\n",
        "\n",
        "MAE & RMSE are extremely low â†’ Predictions are highly accurate.\n",
        "\n",
        "Slightly weaker than Random Forest (Model 2), but still strong.\n",
        "\n",
        "This shows XGBoost is powerful but Random Forest was already near-perfect for this dataset."
      ],
      "metadata": {
        "id": "G4ixyKMQUYSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# VISUALIZING EVALUATION METRIC SCORE CHART (MODEL 3)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Metric values from Model 3 (XGBoost)\n",
        "metrics_3 = {\n",
        "    'MAE': mae_3,\n",
        "    'MSE': mse_3,\n",
        "    'RMSE': rmse_3,\n",
        "    'R2 Score': r2_3\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(metrics_3.keys(), metrics_3.values(), color='orange')\n",
        "\n",
        "plt.title(\"Evaluation Metrics for ML Model 3 (XGBoost Regressor)\")\n",
        "plt.xlabel(\"Metrics\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "# Display values on the bars\n",
        "for key, value in metrics_3.items():\n",
        "    plt.text(key, value, f\"{value:.6f}\", ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# ML MODEL - 3 (XGBOOST) WITH CROSS-VALIDATION & TUNING\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Base model\n",
        "xgb = XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Hyperparameter space for tuning\n",
        "param_dist = {\n",
        "    'n_estimators': [200, 300, 500, 800],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'max_depth': [4, 5, 6, 8, 10],\n",
        "    'subsample': [0.6, 0.7, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.7, 0.8, 1.0],\n",
        "    'min_child_weight': [1, 3, 5, 7]\n",
        "}\n",
        "\n",
        "# Cross-validation\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Randomized Search CV\n",
        "random_search_xgb = RandomizedSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,             # test 20 random combinations\n",
        "    cv=kfold,\n",
        "    scoring='r2',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Fit the Algorithm\n",
        "# --------------------------\n",
        "random_search_xgb.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters Found:\")\n",
        "print(random_search_xgb.best_params_)\n",
        "\n",
        "# Best tuned XGBoost model\n",
        "best_xgb_model = random_search_xgb.best_estimator_\n",
        "\n",
        "# --------------------------\n",
        "# Predict on the model\n",
        "# --------------------------\n",
        "y_pred_xgb_tuned = best_xgb_model.predict(X_test)\n",
        "\n",
        "# --------------------------\n",
        "# Evaluation\n",
        "# --------------------------\n",
        "mae_xgb_tuned = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
        "mse_xgb_tuned = mean_squared_error(y_test, y_pred_xgb_tuned)\n",
        "rmse_xgb_tuned = np.sqrt(mse_xgb_tuned)\n",
        "r2_xgb_tuned = r2_score(y_test, y_pred_xgb_tuned)\n",
        "\n",
        "print(\"\\nTuned XGBoost Model Results:\")\n",
        "print(\"--------------------------------------\")\n",
        "print(\"MAE  :\", mae_xgb_tuned)\n",
        "print(\"MSE  :\", mse_xgb_tuned)\n",
        "print(\"RMSE :\", rmse_xgb_tuned)\n",
        "print(\"RÂ² Score:\", r2_xgb_tuned)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For ML Model-3 (XGBoost Regressor), I used RandomizedSearchCV for hyperparameter optimization.\n",
        "\n",
        "âœ” Why RandomizedSearchCV?\n",
        "1. Efficient for Large Hyperparameter Space\n",
        "\n",
        "XGBoost contains many hyperparameters:\n",
        "\n",
        "n_estimators\n",
        "\n",
        "max_depth\n",
        "\n",
        "learning_rate\n",
        "\n",
        "subsample\n",
        "\n",
        "colsample_bytree\n",
        "\n",
        "min_child_weight\n",
        "\n",
        "Testing every combination using GridSearchCV would be extremely slow.\n",
        "RandomizedSearchCV searches the most useful combinations quickly.\n",
        "\n",
        "2. Faster and More Scalable\n",
        "\n",
        "It evaluates only a subset of random hyperparameter combinations, making it:\n",
        "\n",
        "Much faster\n",
        "\n",
        "More scalable\n",
        "\n",
        "Ideal for XGBoost\n",
        "\n",
        "3. Built-in Cross-Validation\n",
        "\n",
        "It performs 5-Fold CV, which ensures:\n",
        "\n",
        "No overfitting\n",
        "\n",
        "Stable evaluation\n",
        "\n",
        "Reliable hyperparameters\n",
        "\n",
        "4. Finds Near-Optimal Solution Efficiently\n",
        "\n",
        "In many cases, RandomizedSearchCV finds a hyperparameter set that performs as well as or better than GridSearch, but in much less time."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the tuned XGBoost model shows a significant improvement over the original untuned XGBoost model.\n",
        "\n",
        "Here is the comparison:\n",
        "\n",
        "**Before vs After Hyperparameter Tuning â€” Comparison Table**\n",
        "\n",
        "| Metric       | Before Tuning | After Tuning | Improvement               |\n",
        "| ------------ | ------------- | ------------ | ------------------------- |\n",
        "| **MAE**      | 0.01153       | 0.00127      | **Huge improvement â†“**    |\n",
        "| **MSE**      | 0.000373      | 0.000011     | **Massive improvement â†“** |\n",
        "| **RMSE**     | 0.01932       | 0.00333      | **Huge improvement â†“**    |\n",
        "| **RÂ² Score** | 0.999653      | 0.999990     | **Improved â†‘**            |\n",
        "\n",
        "\n",
        "Interpretation\n",
        "\n",
        "MAE decreased by ~89%\n",
        "\n",
        "RMSE decreased by ~83%\n",
        "\n",
        "RÂ² increased from 0.99965 â†’ 0.99999, indicating almost perfect predictions\n",
        "\n",
        "XGBoost benefited highly from tuning, unlike Linear Regression\n",
        "\n",
        "This proves tuning was essential for maximizing performance"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# MODEL 3: BEFORE vs AFTER TUNING METRIC COMPARISON CHART\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "metrics_before = [mae_3, mse_3, rmse_3, r2_3]\n",
        "metrics_after = [mae_xgb_tuned, mse_xgb_tuned, rmse_xgb_tuned, r2_xgb_tuned]\n",
        "labels = ['MAE', 'MSE', 'RMSE', 'R2 Score']\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(x - width/2, metrics_before, width, label='Before Tuning', color='orange')\n",
        "plt.bar(x + width/2, metrics_after, width, label='After Tuning', color='green')\n",
        "\n",
        "plt.title(\"Model 3 (XGBoost): Evaluation Metrics Before vs After Hyperparameter Tuning\")\n",
        "plt.xlabel(\"Metrics\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xticks(x, labels)\n",
        "plt.legend()\n",
        "\n",
        "# Annotate values\n",
        "for i, v in enumerate(metrics_before):\n",
        "    plt.text(i - width/2, v, f\"{v:.6f}\", ha='center', va='bottom')\n",
        "\n",
        "for i, v in enumerate(metrics_after):\n",
        "    plt.text(i + width/2, v, f\"{v:.6f}\", ha='center', va='bottom')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "noWzODDFVubN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I considered the following evaluation metrics because each one directly supports salary prediction accuracy and delivers measurable business value:\n",
        "\n",
        "âœ” Mean Absolute Error (MAE)\n",
        "\n",
        "MAE shows the average salary prediction error in real units.\n",
        "\n",
        "It is easy to interpret and directly tells HR teams how much the predicted salary may deviate from actual salary.\n",
        "\n",
        "Business Impact: Lower MAE = more reliable salary insights â†’ reduces risk of underpaying or overpaying candidates.\n",
        "\n",
        "âœ” Mean Squared Error (MSE)\n",
        "\n",
        "Captures how large the prediction errors are by penalizing larger mistakes.\n",
        "\n",
        "Useful for detecting instability in the model.\n",
        "\n",
        "Business Impact: Helps prevent large deviations in salary prediction, ensuring that budgeting remains accurate and consistent.\n",
        "\n",
        "âœ” Root Mean Squared Error (RMSE)\n",
        "\n",
        "Square root of MSE, interpretable in salary units.\n",
        "\n",
        "Highlights large salary prediction errors, which are critical for compensation planning.\n",
        "\n",
        "Business Impact: Lower RMSE ensures the model avoids big mistakes that could mislead hiring budgets.\n",
        "\n",
        "âœ” RÂ² Score (Coefficient of Determination)\n",
        "\n",
        "Measures how much of the salary variation is explained by the model.\n",
        "\n",
        "Business Impact: High RÂ² means the model captures real market patterns â†’ giving strong confidence to decision-makers like recruiters, HR, and job seekers."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected XGBoost Regressor (Model-3) as the final prediction model.\n",
        "\n",
        "âœ” Why XGBoost was chosen?\n",
        "1. Best Accuracy\n",
        "\n",
        "After tuning:\n",
        "\n",
        "MAE = 0.00127\n",
        "\n",
        "RMSE = 0.00332\n",
        "\n",
        "RÂ² Score = 0.99999\n",
        "\n",
        "This was the best performance among all three models.\n",
        "\n",
        "2. Handles Non-Linear Relationships\n",
        "\n",
        "Salaries depend on complex interactions (job title, experience, skills, location).\n",
        "XGBoost captures these patterns better than Linear Regression or Random Forest.\n",
        "\n",
        "3. Effective with Large Feature Spaces\n",
        "\n",
        "After one-hot encoding + TF-IDF + engineered features, the dataset has many columns.\n",
        "XGBoost is designed for high-dimensional structured data.\n",
        "\n",
        "4. Regularization Avoids Overfitting\n",
        "\n",
        "XGBoost applies L1 + L2 regularization, improving generalization.\n",
        "\n",
        "5. Tuning Improved the Model Significantly\n",
        "\n",
        "The tuned XGBoost model outperformed all other models with the lowest error values."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used XGBoost Regressor, a boosted decision tree-based ensemble model that builds multiple trees sequentially.\n",
        "Each tree corrects the errors of the previous one, making the model highly accurate and robust.\n",
        "\n",
        "â­ Model Explainability Using Feature Importance (XGBoost Built-In Feature Importance)\n",
        "\n",
        "XGBoost provides three types of importance:\n",
        "\n",
        "Weight: Number of times a feature appears in splits\n",
        "\n",
        "Gain: Average improvement in model accuracy when a feature is used\n",
        "\n",
        "Cover: Relative number of samples affected\n",
        "\n",
        "I used Gain-based feature importance, which is the most informative."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# XGBOOST FEATURE IMPORTANCE\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(12,8))\n",
        "xgb.plot_importance(best_xgb_model, max_num_features=20, importance_type='gain')\n",
        "plt.title(\"Top 20 Important Features (XGBoost)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ECa3WvFcWJVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top Important Features (Interpretation)\n",
        "\n",
        "Based on model importance, the most influential features were:\n",
        "\n",
        "1. Job Title Features (e.g., Data Scientist, ML Engineer)\n",
        "\n",
        "Different titles have significantly different salary ranges.\n",
        "\n",
        "2. Company Rating\n",
        "\n",
        "Higher-rated companies offer better compensation.\n",
        "\n",
        "3. Job Location (State/City Encodings)\n",
        "\n",
        "Salaries vary heavily by region (e.g., NY, SF, Seattle pay more).\n",
        "\n",
        "4. Skill-Related TF-IDF Features\n",
        "\n",
        "Keywords like python, machine learning, AWS, SQL had high importance.\n",
        "\n",
        "5. Company Size\n",
        "\n",
        "Larger companies usually offer competitive pay.\n",
        "\n",
        "6. Description Length / Word Count\n",
        "\n",
        "Longer job descriptions typically indicate more responsibilities and higher salaries.\n",
        "\n",
        "â­ Business Impact of Explainability\n",
        "\n",
        "Feature importance helps HR teams, job seekers, and employers understand:\n",
        "\n",
        "What attributes drive salaries the most\n",
        "\n",
        "Which skills increase earning potential\n",
        "\n",
        "How company factors influence pay\n",
        "\n",
        "Why the model predicts a particular salary\n",
        "\n",
        "This increases model transparency, trust, and adoption across business teams."
      ],
      "metadata": {
        "id": "w-Ms4K6JWOr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# SAVE BEST MODEL USING PICKLE\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Save the tuned XGBoost model\n",
        "with open(\"best_xgb_model.pkl\", \"wb\") as file:\n",
        "    pickle.dump(best_xgb_model, file)\n",
        "\n",
        "print(\"Best XGBoost model saved successfully as best_xgb_model.pkl\")"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# SAVE BEST MODEL USING JOBLIB\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import joblib\n",
        "\n",
        "joblib.dump(best_xgb_model, \"best_xgb_model.joblib\")\n",
        "print(\"Best XGBoost model saved successfully as best_xgb_model.joblib\")"
      ],
      "metadata": {
        "id": "rgQ-giNmWbAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# LOAD SAVED MODEL (PICKLE) & PREDICT UNSEEN DATA\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Load the saved model\n",
        "with open(\"best_xgb_model.pkl\", \"rb\") as file:\n",
        "    loaded_model = pickle.load(file)\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# CREATE SAMPLE UNSEEN DATA INPUT\n",
        "# (Must match your training feature order & preprocessing)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Example unseen feature vector (replace with actual values)\n",
        "# Make sure the feature count matches X_train.shape[1]\n",
        "\n",
        "sample_unseen = np.array([X_train.iloc[0].values])   # using a valid row format\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# PREDICT USING LOADED MODEL\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "predicted_salary = loaded_model.predict(sample_unseen)\n",
        "print(\"Predicted Salary for Unseen Data:\", predicted_salary[0])"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# LOAD SAVED MODEL (JOBLIB) & PREDICT UNSEEN DATA\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "# Load the joblib model\n",
        "loaded_model = joblib.load(\"best_xgb_model.joblib\")\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Create an unseen sample (same format as X_train)\n",
        "sample_unseen = np.array([X_train.iloc[1].values])\n",
        "\n",
        "# Predict\n",
        "predicted_salary = loaded_model.predict(sample_unseen)\n",
        "print(\"Predicted Salary for Unseen Data:\", predicted_salary[0])"
      ],
      "metadata": {
        "id": "7vDWCvXOWojd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}